<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[PyQt5使用小结]]></title>
    <url>%2F2017%2F11%2F12%2FPyQt5%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文所用的Python版本为python-3.6.2，PyQt5版本为pyqt5-5.9.1，OpenCV版本为opencv-python-3.3.0.10和opencv-contrib-python-3.3.0.10，TensorFlow版本为tensorflow-1.4.0，编程语言为python3，系统环境为Windows 10。 前言 本文是上一篇（TensorFlow Object Detection API使用小结）的后续，因为那个project还需要一个界面，所以本人使用PyQt做了这么个界面，其中借用OpenCV的图像数据显示。 准备篇 首先使用pip安装所需库，由于上一篇已经安装了tensorflow，所以本文其实只需要安装pyqt5和opencv-python就可以了，安装pyqt5指令为：pip install pyqt5，相关依赖关系解决办法在上一篇中已提到，这里不再赘述，然后再使用指令pip install opencv-python安装opencv，这里本人发现在python中配置OpenCV简直不要太轻爽O(∩_∩)O~~，就一个pip就解决了，哪有C++那么麻烦，以后可能还会继续使用python版的opencv，所以就顺便把它python版的扩展包也顺便一起装上，安装指令为：pip install opencv-contrib-python。至此所需环境库安装完毕。 *注：相对于上文中使用pip在线安装的方式，还有另一种使用pip进行离线安装的方式，在Unofficial Windows Binaries for Python Extension Packages上下载离线包，即XXXXXX.whl文件，文件名一般包含库名称和对应版本、python版本以及是64位还是32位的等信息，这里以离线包numpy-1.13+mkl为例，首先下载适合自己的库版本，适合本人的当然是numpy-1.13.3+mkl-cp36-cp36m-win_amd64.whl（这适合64位的python3.6安装），将命令行目录切换至numpy-1.13.3+mkl-cp36-cp36m-win_amd64.whl文件所在目录，输入指令pip install numpy-1.13.3+mkl-cp36-cp36m-win_amd64.whl即可离线安装numpy-1.13+mkl库。相比在线安装，这种离线安装更加灵活，而且能够安装一些在线安装无法安装的库，像上文中的numpy-1.13+mkl库只能采取离线安装的方式，在线安装只能安装不带mkl的numpy库。采用离线安装方式也可以直接安装带扩展包的opencv-python库：opencv_python‑3.3.1+contrib‑cp36‑cp36m‑win_amd64.whl ，不需要像在线安装那样装两个库。 实践篇 以前有用过Qt的基础，所以这次使用PyQt5感觉上手很快，毕竟这里面的语法有很多是相通的，再加上网上的资料也有很多，所以很快就做了个简陋的界面。不过直接用代码控制界面的布局确实很麻烦，每改下布局都要重新运行一下看看，而且启动时间还有点长╮(╯﹏╰）╭。网上有种说法是： 可以先通过QtDesigner设计UI，然后通过Qt提供的命令行工具pyuic5将.ui文件转换成python代码，具体用法是：若ui文件名称为firstPyQt5.ui，则在命令行界面中输入指令：pyuic5 -o firstPyQt5.py firstPyQt5.ui，即可将firstPyQt5.ui文件转换成python代码文件firstPyQt5.py 不过本人这里由于界面比较简陋，没有几个控件，所以就直接将其用python代码控制了，没去尝试这个命令行工具pyuic5了，下次有机会再尝试吧↖(&#94;ω^)↗。 本人做的这个小界面实现的功能是：1、可以选择已经训练好的模型来检测选定图片中的目标；2、可以播放选定的视频；3、还有打开摄像头，显示摄像头拍摄的视频。 其中由于本人电脑无法实时检测目标，所以在视频和摄像头拍摄中就没有添加检测的代码，只有选择图片时才会执行检测功能，有需要的童靴可以自行添加(•̀ᴗ•́)。 附完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193import osimport sys from PyQt5.QtCore import * from PyQt5.QtWidgets import *from PyQt5.QtGui import * import numpy as npimport cv2import tensorflow as tffrom object_detection.utils import label_map_utilfrom object_detection.utils import visualization_utils as vis_utilclass Detector(object): def __init__(self): self.PATH_TO_CKPT = './model/hand_model_faster_rcnn_resnet101.pb' # 选择模型文件 self.PATH_TO_LABELS = r'./model/hands_label_map.pbtxt' # 选择类别标签文件 self.NUM_CLASSES = 1 self.detection_graph = self._load_model() # 加载模型 self.category_index = self._load_label_map() def _load_model(self): detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile(self.PATH_TO_CKPT, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') return detection_graph def _load_label_map(self): label_map = label_map_util.load_labelmap(self.PATH_TO_LABELS) categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=self.NUM_CLASSES, use_display_name=True) category_index = label_map_util.create_category_index(categories) return category_index def detect(self, image): with self.detection_graph.as_default(): with tf.Session(graph=self.detection_graph) as sess: # Expand dimensions since the model expects images to have shape: [1, None, None, 3] image_np_expanded = np.expand_dims(image, axis=0) image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0') boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0') scores = self.detection_graph.get_tensor_by_name('detection_scores:0') classes = self.detection_graph.get_tensor_by_name('detection_classes:0') num_detections = self.detection_graph.get_tensor_by_name('num_detections:0') # Actual detection. (boxes, scores, classes, num_detections) = sess.run( [boxes, scores, classes, num_detections], feed_dict=&#123;image_tensor: image_np_expanded&#125;) # Visualization of the results of a detection. vis_util.visualize_boxes_and_labels_on_image_array( image, np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores), self.category_index, use_normalized_coordinates=True, line_thickness=8) return imageclass DetectUI(QWidget): def __init__(self): super().__init__() self.initUI() self.detector = Detector() self.cap = cv2.VideoCapture() def initUI(self): self.timer = QTimer(self) # 初始化一个定时器 self.timer.timeout.connect(self.showFrame) # 计时结束调用showFrame()方法 self.show_pic_label = QLabel(self) self.show_pic_label.resize(640, 480) self.show_pic_label.move(10, 10) self.show_pic_label.setStyleSheet("border-width: 1px; border-style: solid; border-color: rgb(255, 170, 0);") self.show_filename_lineEdit = QLineEdit(self) self.show_filename_lineEdit.resize(200, 22) self.show_filename_lineEdit.move(10, 500) self.select_img_btn = QPushButton('Select File', self) self.select_img_btn.clicked.connect(self.selectImg) self.select_img_btn.resize(self.select_img_btn.sizeHint()) self.select_img_btn.move(218, 500) self.open_camera_btn = QPushButton('Open Camera', self) self.open_camera_btn.clicked.connect(self.openCamera) self.open_camera_btn.resize(self.open_camera_btn.sizeHint()) self.open_camera_btn.move(292, 500) self.select_model_btn = QPushButton('Select Model', self) self.select_model_btn.clicked.connect(self.selectModel) self.select_model_btn.resize(self.select_model_btn.sizeHint()) self.select_model_btn.move(366, 500) self.show_modelname_lineEdit = QLineEdit(self) self.show_modelname_lineEdit.setText('hand_model_faster_rcnn_resnet101.pb') self.show_modelname_lineEdit.resize(200, 22) self.show_modelname_lineEdit.move(450, 500) self.setGeometry(200, 100, 660, 530) self.setWindowTitle('Hand Detector') self.show() def showImg(self, src_img, qlabel): src_img = cv2.cvtColor(src_img, cv2.COLOR_BGR2RGB) height, width, bytesPerComponent = src_img.shape bytesPerLine = bytesPerComponent * width # 转为QImage对象 q_image = QImage(src_img.data, width, height, bytesPerLine, QImage.Format_RGB888) qlabel.setPixmap(QPixmap.fromImage(q_image).scaled(qlabel.width(), qlabel.height())) def showFrame(self): if(self.cap.isOpened()): ret, frame = self.cap.read() if ret: self.showImg(frame, self.show_pic_label) else: self.cap.release() self.timer.stop() # 停止计时器 def selectImg(self): if self.cap.isOpened(): self.cap.release() file_name, file_type = QFileDialog.getOpenFileName(self, "选取文件", "./", "Image Files (*.jpg *.png *.bmp *.tif);;Video Files (*.avi *.mp4)") #设置文件扩展名过滤,注意用双分号间隔过滤，用空格分隔多个文件 # print(file_name,file_type) if file_type.find("Image") &gt;= 0: if file_name: self.show_filename_lineEdit.setText(os.path.split(file_name)[1]) img = cv2.imread(file_name, cv2.IMREAD_COLOR) cv2.cvtColor(img, cv2.COLOR_BGR2RGB, img) img = self.detector.detect(img) # 检测目标 height, width, bytesPerComponent = img.shape bytesPerLine = bytesPerComponent * width # 转为QImage对象 q_image = QImage(img.data, width, height, bytesPerLine, QImage.Format_RGB888) self.show_pic_label.setPixmap(QPixmap.fromImage(q_image).scaled(self.show_pic_label.width(), self.show_pic_label.height())) if file_type.find("Video") &gt;= 0: if file_name: self.show_filename_lineEdit.setText(os.path.split(file_name)[1]) self.cap.open(file_name) self.timer.start(30) # 设置时间隔30ms并启动 def openCamera(self): self.cap.open(0) # 默认打开0号摄像头 self.timer.start(30) # 设置时间隔30ms并启动 def selectModel(self): model_name, file_type = QFileDialog.getOpenFileName(self, "选取文件", "./", "model Files (*.pb);;All Files (*)") #设置文件扩展名过滤,注意用双分号间隔过滤，用空格分隔多个文件 if model_name: self.show_modelname_lineEdit.setText(os.path.split(model_name)[1]) self.detector.PATH_TO_CKPT = model_name self.detector.detection_graph = self.detector._load_model() # 重新加载模型 if __name__ == '__main__': app = QApplication(sys.argv) dtcui = DetectUI() sys.exit(app.exec_()) 后记 初次使用Python做一个小东西，其语法确实简洁，不过对于本人这种习惯用C++的人来说确实还有点不太习惯(˘•ω•˘)。 参考资料[1] 用PyQt5+Caffe+Opencv搭建一个人脸识别登录界面 [2] PyQt5学习笔记09—-标准文件打开保存框QFileDialog [3] PyQt5教程——第一个程序（2）（http://www.cnblogs.com/archisama/tag/PyQt5/ ） [4] PyQt5应用与实践 [5] PyQt5系列教程(二)利用QtDesigner设计UI界面（http://www.cnblogs.com/tkinter/tag/pyqt5/ ） [6] OpenCV 3.2.0/OpenCV-Python Tutorials/Gui Features in OpenCV/Getting Started with Images [7] OpenCV 3.2.0/OpenCV-Python Tutorials/Gui Features in OpenCV/Getting Started with Videos [8] python3.3 分割路径与文件名 小例]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>python</tag>
        <tag>qt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow Object Detection API使用小结]]></title>
    <url>%2F2017%2F11%2F11%2FTensorFlow-Object-Detection-API%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[本文所用的Python版本为python-3.6.2，TensorFlow版本为tensorflow-1.4.0，编程语言为python3，系统环境为Windows 10。 前言 很久没写过东西了，主要原因是最近研究生课程开始陆续结课，本人也要忙于应付各种结课时的考试、论文、project等一堆麻烦事。这不深度学习结课时需要做个project，本人也顺便将做这个project的过程记录下来。 准备篇 该project主要利用TensorFlow中的Object Detection API进行训练和检测。在开始使用该API之前需要安装配置Python环境。 既然是Python首先需要下载安装Python，安装完之后，为了顺利使用pip需要配置环境变量，在Windows系统环境变量中Path末尾添加： 变量名 变量值 Path ;C:\Users\admin\AppData\Local\Programs\Python\Python36\;C:\Users\admin\AppData\Local\Programs\Python\Python36\Scripts\ 其中C:\Users\admin\AppData\Local\Programs\Python\Python36为python-3.6.2默认安装目录。 然后为了方便使用命令行工具，下载安装git，安装方式一路默认即可。 接下来利用pip安装TensorFlow，鼠标右键桌面空白处，点击“Git Bash Here”，打开bash命令行，输入pip install tensorflow，其中一些依赖关系可能需要手动解决，手动解决的办法就是用pip install 相关依赖库，这是CPU版的TensorFlow，若要使用GPU，则需要安装GPU版的TensorFlow，安装命令为：pip install tensorflow-gpu，以同样方式解决依赖关系。由于本人电脑没N卡，所以没安装GPU版的TensorFlow，所以如果想使用GPU版的TensorFlow请另行尝试。 然后安装TensorFlow Object Detection API依赖库，在命令行中输入： 1234pip install pillowpip install lxmlpip install jupyterpip install matplotlib 因为tensorflow并没有默认自带Object Detection API，所以该API需要自行下载，下载地址为：https://github.com/tensorflow/models ，下载之后解压，本人解压目录为：D:\ProgramFiles\PythonLibs\tensorflow，解压完之后需要配置环境目录，在系统环境目录中添加： 变量名 变量值 PYTHONPATH D:\ProgramFiles\PythonLibs\tensorflow\models;D:\ProgramFiles\PythonLibs\tensorflow\models\research;D:\ProgramFiles\PythonLibs\tensorflow\models\research\slim; 下载配置Object Detection API完之后需要安装Protoc，进入Protoc下载页，下载protoc-3.4.0-win32.zip，解压之后将bin文件夹内的protoc.exe拷贝到C:\windows\system32目录下（用于将protoc.exe所在的目录配置到环境变量当中）,当然也可以在系统环境变量Path中添加该bin文件夹路径。 最后在命令行中切换目录至：D:\ProgramFiles\PythonLibs\tensorflow\models\research文件夹，即object_detection文件夹所在目录，在命令行中输入： 1protoc object_detection/protos/*.proto --python_out=. 编译object_detection/protos文件夹下的proto文件，生成对应的python文件。 至此，Windows下TensorFlow中的Object Detection API的使用配置全部完成，至于Ubuntu下的配置可参考其官方文档。 至于如何验证，可以在命令行中切换目录至object_detection，输入：jupyter notebook，稍等一会，浏览器将自动打开http://localhost:8888/tree jupyter界面，点击object_detection_tutorial.ipynb文件，进入打开的新标签，点击“Cell”中的“Run All”，耐心等待几 ~ 十几分钟（因为它需要下载相应的模型），将会在浏览器下方显示检测结果。 截止本文完成前，该API公开的有以下几个模型： Model name Speed (ms) COCO mAP[^1] Outputs ssd_mobilenet_v1_coco 30 21 Boxes ssd_inception_v2_coco 42 24 Boxes faster_rcnn_inception_v2_coco 58 28 Boxes faster_rcnn_resnet50_coco 89 30 Boxes faster_rcnn_resnet50_lowproposals_coco 64 Boxes rfcn_resnet101_coco 92 30 Boxes faster_rcnn_resnet101_coco 106 32 Boxes faster_rcnn_resnet101_lowproposals_coco 82 Boxes faster_rcnn_inception_resnet_v2_atrous_coco 620 37 Boxes faster_rcnn_inception_resnet_v2_atrous_lowproposals_coco 241 Boxes faster_rcnn_nas 1833 43 Boxes faster_rcnn_nas_lowproposals_coco 540 Boxes 根据上述模型可推知，利用该API可能只能训练Faster-RCNN、R-FCN和SSD三种算法的模型。 接下来介绍如何使用该API来训练自己的模型进行物体检测。 实践篇数据准备篇 既然要训练自己的模型，当然要准备相应的数据，而TensorFlow有其独特的输入数据格式TFRecord，所以通常还要将自己的数据转换成TFRecord格式以输入TensorFlow中进行训练。以datitran/raccoon_dataset数据集为例，该作者在Google image上收集了200张Raccoon图片，用LabelImg对这些图片进行标记，并将标记以PASCAL VOC格式保存为xml文件。作者在文中也提到了另一个图片标记工具FIAT (Fast Image Data Annotation Tool)。保存为PASCAL VOC格式的xml文件之后，可以使用object_detection文件夹中的create_pascal_tf_record.py文件将数据转化为TFRecord格式，用法为： 123./create_pascal_tf_record --data_dir=/home/user/VOCdevkit \ --year=VOC2012 \ --output_path=/home/user/pascal.record 当然也可以使用datitran作者提供的xml_to_csv.py文件将xml文件先转化为csv文件，再利用generate_tfrecord.py文件将csv文件转化成TFRecord格式文件。 注意，使用xml_to_csv.py和generate_tfrecord.py其文件结构应该是这样的： .├── annotations├── generate_tfrecord.py├── images└── xml_to_csv.py 2 directories, 2 files 其中images文件夹存的是jpg图片，annotations文件夹存的是xml标签文件。generate_tfrecord.py文件中的： 12345def class_text_to_int(row_label): if row_label == 'raccoon': return 1 else:None 其中的raccoon注意要改成自己的类别标签。如此，数据的问题就解决了。 训练篇 然后就是正式开始训练了，以Faster-RCNN算法为例。首先准备相应的数据，本人准备的数据文件结构如下： TensorFlowObjectDetectionAPITest├── data│ ├── model.ckpt.data-00000-of-00001│ ├── model.ckpt.index│ ├── model.ckpt.meta│ ├── object_label_map.pbtxt│ ├── test.record│ └── train.record├── eval├── eval.py├── export_inference_graph.py├── faster_rcnn_resnet101_coco.config├── model├── train└── train.py 4 directories, 10 files 其中，TensorFlowObjectDetectionAPITest为项目文件夹，该project在此文件夹下运行； data文件夹中三个model.ckpt文件：model.ckpt.data-00000-of-00001、model.ckpt.index和model.ckpt.meta来自faster_rcnn_resnet101_coco模型，用来初始化网络参数； object_label_map.pbtxt文件内容如下： item {​ id: 1​ name: ‘raccoon’} 将其中的raccoon改成自己的类别标签，如果有多个类别标签则可以参考object_detection\data文件夹中的pascal_label_map.pbtxt文件格式； test.record和train.record是生成的TFRecord数据，分别为待输入的测试数据和训练数据； eval文件夹为空文件夹用来输出测试结果；train文件夹为空文件夹用来输出训练结果（包括checkpoint文件和最终的模型文件）； faster_rcnn_resnet101_coco.config为配置文件，包括各种参数和输入输出数据的配置，其来自object_detection\samples\configs文件夹中faster_rcnn_resnet101_coco.config文件，在使用时需对其做如下修改： 首先是num_classes，这是待检测的类别数目，如果只要检测一种，则将其值改为1； fine_tune_checkpoint: &quot;PATH_TO_BE_CONFIGURED/model.ckpt&quot;，将PATH_TO_BE_CONFIGURED改为./data； 123456train_input_reader: &#123; tf_record_input_reader &#123; input_path: &quot;PATH_TO_BE_CONFIGURED/mscoco_train.record&quot; &#125; label_map_path: &quot;PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt&quot;&#125; 将其中的的PATH_TO_BE_CONFIGURED/mscoco_train.record改为./data/train.record，将其中的PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt改为./data/object_label_map.pbtxt； 123456789eval_input_reader: &#123; tf_record_input_reader &#123; input_path: &quot;PATH_TO_BE_CONFIGURED/mscoco_val.record&quot; &#125; label_map_path: &quot;PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt&quot; shuffle: false num_readers: 1 num_epochs: 1&#125; 将其中的的PATH_TO_BE_CONFIGURED/mscoco_val.record改为./data/test.record，将其中的PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt改为./data/object_label_map.pbtxt； 至于其它的参数可以选择默认，不对其进行修改； train.py为训练代码，其来自object_detection/文件夹中的train.py，直接复制出来使用即可，具体用法为： 1python train.py --logtostderr --train_dir=./train --pipeline_config_path=faster_rcnn_resnet101_coco.config 其在运行过程中会在train文件夹生成一系列训练过程文件，比如checkpoint、model.ckpt-{num}（{num}代表训练过程保存的第几个网络模型，一般从0开始，包括.index、.meta和.data三个文件）等文件。 eval.py为评估代码，其来自object_detection/文件夹中的eval.py，直接复制出来使用即可，具体用法为： 1python eval.py --logtostderr --checkpoint_dir=./train --eval_dir=./eval --pipeline_config_path=./faster_rcnn_resnet101_coco.config 其在运行过程中会在eval文件夹生成一系列评估文件，每个文件对应一个测试image。 export_inference_graph.py为导出pb模型代码，其来自object_detection/文件夹中的export_inference_graph.py，直接复制出来使用即可，具体用法为： 1python export_inference_graph.py --input_type image_tensor --pipeline_config_path ./faster_rcnn_resnet101_coco.config --trained_checkpoint_prefix ./train/model.ckpt-18298 --output_directory ./model 其中model.ckpt-18298表示使用第18298次保存的网络模型导出pb模型文件，导出的模型文件保存在model文件夹，主要有一下几个文件： - graph.pbtxt - model.ckpt.data-00000-of-00001 - model.ckpt.info - model.ckpt.meta - frozen_inference_graph.pb 其中frozen_inference_graph.pb就是训练成功用来检测目标的模型。 TensorFlow训练时可以随时查看训练过程，如损失函数的值下降曲线等，所用命令为：在命令行中切换目录至project运行目录，即train.py所在文件夹，本人这里即TensorFlowObjectDetectionAPITest文件夹，输入：tensorboard --logdir=./，等待片刻，在浏览器地址栏输入：http://localhost:6006/，即可看到训练过程曲线。 检测篇 检测结果使用opencv窗口显示（至于python中opencv的使用请见下一篇），具体调用自己训练的模型进行检测的Python代码（该代码为eli大佬参考object_detection文件夹中的object_detection_tutorial.ipynb（该文件可在jupyter中查看）改的）为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import cv2import numpy as npimport tensorflow as tffrom object_detection.utils import label_map_utilfrom object_detection.utils import visualization_utils as vis_utilclass Detector(object): def __init__(self): self.PATH_TO_CKPT = r'./model/frozen_inference_graph.pb' # 选择模型 self.PATH_TO_LABELS = r'./data/object_label_map.pbtxt' # 选择类别标签文件 self.NUM_CLASSES = 1 self.detection_graph = self._load_model() self.category_index = self._load_label_map() def _load_model(self): detection_graph = tf.Graph() with detection_graph.as_default(): od_graph_def = tf.GraphDef() with tf.gfile.GFile(self.PATH_TO_CKPT, 'rb') as fid: serialized_graph = fid.read() od_graph_def.ParseFromString(serialized_graph) tf.import_graph_def(od_graph_def, name='') return detection_graph def _load_label_map(self): label_map = label_map_util.load_labelmap(self.PATH_TO_LABELS) categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=self.NUM_CLASSES, use_display_name=True) category_index = label_map_util.create_category_index(categories) return category_index def detect(self, image): with self.detection_graph.as_default(): with tf.Session(graph=self.detection_graph) as sess: # Expand dimensions since the model expects images to have shape: [1, None, None, 3] image_np_expanded = np.expand_dims(image, axis=0) image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0') boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0') scores = self.detection_graph.get_tensor_by_name('detection_scores:0') classes = self.detection_graph.get_tensor_by_name('detection_classes:0') num_detections = self.detection_graph.get_tensor_by_name('num_detections:0') # Actual detection. (boxes, scores, classes, num_detections) = sess.run( [boxes, scores, classes, num_detections], feed_dict=&#123;image_tensor: image_np_expanded&#125;) # Visualization of the results of a detection. vis_util.visualize_boxes_and_labels_on_image_array( image, np.squeeze(boxes), np.squeeze(classes).astype(np.int32), np.squeeze(scores), self.category_index, use_normalized_coordinates=True, line_thickness=8) cv2.namedWindow("detection", cv2.WINDOW_NORMAL) cv2.imshow("detection", image) cv2.waitKey(0)if __name__ == '__main__': image = cv2.imread('./test_img.jpg') # 选择待检测的图片 detector = Detector() detector.detect(image) 后记 经过这次TensorFlow训练，感觉深度学习真tm吃硬件，费时间，也难怪神经网络理论出来几十年之后才火，当年的硬件根本无法支持这么大的计算量。 附录最后附上datitran作者提供的xml_to_csv.py文件源码和generate_tfrecord.py文件源码： xml_to_csv.py源码如下： 1234567891011121314151617181920212223242526272829303132333435import osimport globimport pandas as pdimport xml.etree.ElementTree as ETdef xml_to_csv(path): xml_list = [] for xml_file in glob.glob(path + '/*.xml'): tree = ET.parse(xml_file) root = tree.getroot() for member in root.findall('object'): value = (root.find('filename').text, int(root.find('size')[0].text), int(root.find('size')[1].text), member[0].text, int(member[4][0].text), int(member[4][1].text), int(member[4][2].text), int(member[4][3].text) ) xml_list.append(value) column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax'] xml_df = pd.DataFrame(xml_list, columns=column_name) return xml_dfdef main(): image_path = os.path.join(os.getcwd(), 'annotations') xml_df = xml_to_csv(image_path) xml_df.to_csv('raccoon_labels.csv', index=None) print('Successfully converted xml to csv.')main() generate_tfrecord.py文件源码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798"""Usage: # From tensorflow/models/ # Create train data: python generate_tfrecord.py --csv_input=data/train_labels.csv --output_path=train.record # Create test data: python generate_tfrecord.py --csv_input=data/test_labels.csv --output_path=test.record"""from __future__ import divisionfrom __future__ import print_functionfrom __future__ import absolute_importimport osimport ioimport pandas as pdimport tensorflow as tffrom PIL import Imagefrom object_detection.utils import dataset_utilfrom collections import namedtuple, OrderedDictflags = tf.app.flagsflags.DEFINE_string('csv_input', '', 'Path to the CSV input')flags.DEFINE_string('output_path', '', 'Path to output TFRecord')FLAGS = flags.FLAGS# TO-DO replace this with label mapdef class_text_to_int(row_label): if row_label == 'raccoon': return 1 else: Nonedef split(df, group): data = namedtuple('data', ['filename', 'object']) gb = df.groupby(group) return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]def create_tf_example(group, path): with tf.gfile.GFile(os.path.join(path, '&#123;&#125;'.format(group.filename)), 'rb') as fid: encoded_jpg = fid.read() encoded_jpg_io = io.BytesIO(encoded_jpg) image = Image.open(encoded_jpg_io) width, height = image.size filename = group.filename.encode('utf8') image_format = b'jpg' xmins = [] xmaxs = [] ymins = [] ymaxs = [] classes_text = [] classes = [] for index, row in group.object.iterrows(): xmins.append(row['xmin'] / width) xmaxs.append(row['xmax'] / width) ymins.append(row['ymin'] / height) ymaxs.append(row['ymax'] / height) classes_text.append(row['class'].encode('utf8')) classes.append(class_text_to_int(row['class'])) tf_example = tf.train.Example(features=tf.train.Features(feature=&#123; 'image/height': dataset_util.int64_feature(height), 'image/width': dataset_util.int64_feature(width), 'image/filename': dataset_util.bytes_feature(filename), 'image/source_id': dataset_util.bytes_feature(filename), 'image/encoded': dataset_util.bytes_feature(encoded_jpg), 'image/format': dataset_util.bytes_feature(image_format), 'image/object/bbox/xmin': dataset_util.float_list_feature(xmins), 'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs), 'image/object/bbox/ymin': dataset_util.float_list_feature(ymins), 'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs), 'image/object/class/text': dataset_util.bytes_list_feature(classes_text), 'image/object/class/label': dataset_util.int64_list_feature(classes), &#125;)) return tf_exampledef main(_): writer = tf.python_io.TFRecordWriter(FLAGS.output_path) path = os.path.join(os.getcwd(), 'images') examples = pd.read_csv(FLAGS.csv_input) grouped = split(examples, 'filename') for group in grouped: tf_example = create_tf_example(group, path) writer.write(tf_example.SerializeToString()) writer.close() output_path = os.path.join(os.getcwd(), FLAGS.output_path) print('Successfully created the TFRecords: &#123;&#125;'.format(output_path))if __name__ == '__main__': tf.app.run() 参考资料[1] 对于谷歌开源的TensorFlow Object Detection API视频物体识别系统实现教程 [2] TensorFlow学习——Tensorflow Object Detection API（win10，CPU） [3] How to train your own Object Detector with TensorFlow’s Object Detector API [4] TensorFlow 之 物体检测（http://rensanning.iteye.com/category/374992 ）]]></content>
      <categories>
        <category>Study</category>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C语言中整型提升问题]]></title>
    <url>%2F2017%2F10%2F27%2FC%E8%AF%AD%E8%A8%80%E4%B8%AD%E6%95%B4%E5%9E%8B%E6%8F%90%E5%8D%87%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言 今天有人问了本人一个移位的问题，就是下面这段C语言代码： 12unsigned short a = 0xffff;printf("%#hx\n", a &lt;&lt; 4 &gt;&gt; 8 &lt;&lt; 4); 你认为会输出什么结果？੧ಡ ⌣ ಡ੭ 解答篇 正确答案是：0xfff0。恐怕有一部分会像本人一样觉得答案就是0xff0才对，还像模像样的给出对应的说法：看a首先向左移四位，即去掉最左边的f，右边补4个0变成这样0xfff0；然后再向右移8位，a将会变成这样0x00ff；最后向左移四位，得到0x0ff0，所以应该输出0xff0。但是，正确答案终究是正确答案。之所以会输出正确答案，是因为这里面还有一个整型提升。所谓的整型提升就是： 在一个表达式中，如果int能够表示原始类型中的所有数值，那么这个数值就被转成int型，否则，它被转成unsigned int型。这种规则被称为整型提升。所有其它类型都不会被整型提升改变。 所以在a &lt;&lt; 4 &gt;&gt; 8 &lt;&lt; 4中，会先将a提升为int型，即a会变成0x0000ffff，接着向左移四位，a变成0x000ffff0，再向右移8位，变成0x00000fff，最后向左移4位，变成0x0000fff0，最后为了输出，再做一个隐式的类型转换（由int转unsigned short），得到0xfff0，所以最后输出0xfff0。 后记 这个问题是一个刚入大学的童靴问本人的，刚问本人时本人还没反应过来，后来才想起有整型提升这么回事o(╯□╰)o。btw，这位童靴主要是想去掉高4位和低4位只取中间8位的值，其实最简单的办法就是直接a &amp; 0x0ff0，这样管它有没有整型提升，肯定能得到中间8位的值(╯▽╰)。 参考资料[1] C语言进阶：整型提升（http://blog.csdn.net/mishifangxiangdefeng/article/category/1058873 ） [2] 对 unsigned char 先左移 后右移 可以出现两种结果]]></content>
      <categories>
        <category>Problems</category>
      </categories>
      <tags>
        <tag>c/cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TXT数据转OpenCV中的Mat数据]]></title>
    <url>%2F2017%2F10%2F19%2FTXT%E6%95%B0%E6%8D%AE%E8%BD%ACOpenCV%E4%B8%AD%E7%9A%84Mat%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[前言 本文是以前做的一个小东西的处理前奏，当时也记录过，现在把它翻出来重新看看。那个东西需要利用深度图，本人当时还没拿到Kinect，就在网上下了一些数据（http://eeeweba.ntu.edu.sg/computervision/people/home/renzhou/HandGesture.htm），该数据集包含了彩色图及对应的深度图，但是该数据集没有以图像形式存储深度值，而是用txt文本以行列形式存储真正的深度值（单位为mm），所以并不能直观的看到深度图像，本人需要把这些深度值从txt文本提取出来并把它以图像的形式呈现出来，由于需求比较特殊，网上没看到现成的解决的方案，所以本人只有用现成的轮子自己做一个了。 思路篇 程序的基本思路是：先找到目录及子目录下的所有txt文件路径；再根据路径分别读取txt文件，按行读取之后再进行字符串分割提取其中的深度值；为了便于以图像形式显示，将深度值归一化至0~255存入8位单通道的Mat类型数据中，最后以png图像形式保存至各个目录。 实现篇 因为当时还在用opencv-2.4.11，所以本文所实现的代码是基于opencv-2.4.11，不过应该只要在opencv-2.0版本及以上只要有Mat数据结构的都能用，毕竟本人只用到了OpenCV中的Mat数据结构。Talk is cheap, show you the code（代码很乱，估计也只用这么一次，所以就没怎么注意了:-P）。具体C++实现代码为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279#include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt;#include &lt;io.h&gt; #include &lt;direct.h&gt; #include &lt;fstream&gt;#include &lt;iostream&gt;using namespace cv;using namespace std;// ******************************************************************// @refer to [C++文件读写操作（二）逐字符读取文本和逐行读取文本](http://blog.csdn.net/wangshihui512/article/details/8921924)// [字符串分割(C++)](http://www.cnblogs.com/MikeZhang/archive/2012/03/24/MySplitFunCPP.html)// [C++读取文件夹中所有的文件或者是特定后缀的文件](http://blog.csdn.net/adong76/article/details/39432467)// [C/C++ 判断文件夹是否存在以及创建、删除文件夹 windows以及linux通用](http://blog.csdn.net/u012005313/article/details/50688257)// [Split a string in C++?](http://stackoverflow.com/questions/236129/split-a-string-in-c)// [Kinect开发学习笔记之（六）带游戏者ID的深度数据的提取](http://blog.csdn.net/zouxy09/article/details/8151044)// [Depth Map Tutorial](http://www.pages.drexel.edu/~nk752/depthMapTut.html)// ******************************************************************// ----- 逐个字符读取文件 --------void testByChar() &#123; fstream testByCharFile; char c; testByCharFile.open("./test.txt",ios::in); while(!testByCharFile.eof()) &#123; testByCharFile&gt;&gt;c; cout&lt;&lt;c; &#125; testByCharFile.close(); &#125; // -------- 逐行读取文件 -------------------void testByLine() &#123; char buffer[256]; fstream outFile; outFile.open("./test.txt",ios::in); while(!outFile.eof()) &#123; outFile.getline(buffer, 256, '\n');//getline(char *,int,char) 表示该行字符达到256个或遇到换行就结束 cout&lt;&lt;buffer&lt;&lt;endl; &#125; outFile.close(); &#125; // ------- 分割字符串 --------------void splitString() &#123; char buffer[1280]; fstream outFile; outFile.open("./test.txt",ios::in); while(!outFile.eof()) &#123; outFile.getline(buffer, 1280, '\n');//getline(char *,int,char) 表示该行字符达到1280个或遇到换行就结束 cout&lt;&lt;buffer&lt;&lt;endl; const char *d = " ,*"; char *p; p = strtok(buffer, d); while(p) &#123; printf("%s\n", p); p=strtok(NULL, d); &#125; &#125; outFile.close(); &#125; // 获取文件夹下指定格式所有文件名void getAllFormatFiles( string path, string format, vector&lt;string&gt;&amp; files ) &#123; //文件句柄 long hFile = 0; //文件信息 struct _finddata_t fileinfo; string pathName; if((hFile = _findfirst(pathName.assign(path).append("/*." + format).c_str(),&amp;fileinfo)) != -1) &#123; do &#123; //如果是目录,迭代之 //如果不是,加入列表 if((fileinfo.attrib &amp; _A_SUBDIR)) &#123; if(strcmp(fileinfo.name,".") != 0 &amp;&amp; strcmp(fileinfo.name,"..") != 0) &#123; //files.push_back(p.assign(path).append("/").append(fileinfo.name) ); getAllFormatFiles( pathName.assign(path).append("/").append(fileinfo.name), format, files); &#125; &#125; else &#123; files.push_back(pathName.assign(path).append("/").append(fileinfo.name) ); &#125; &#125;while(_findnext(hFile, &amp;fileinfo) == 0); _findclose(hFile); &#125; &#125; // http://stackoverflow.com/questions/236129/split-a-string-in-c// ---- stackoverflow上大神的C++版本分割字符串 --------------------std::vector&lt;std::string&gt; split(const std::string&amp; text, const std::string&amp; delims)&#123; std::vector&lt;std::string&gt; tokens; std::size_t start = text.find_first_not_of(delims), end = 0; while((end = text.find_first_of(delims, start)) != std::string::npos) &#123; tokens.push_back(text.substr(start, end - start)); start = text.find_first_not_of(delims, end); &#125; if(start != std::string::npos) tokens.push_back(text.substr(start)); return tokens;&#125;// 创建文件夹及子文件夹void makeDir(const string &amp;path)&#123; std::vector&lt;std::string&gt; tokens; std::size_t start = 0, end = 0; while ((end = path.find('/', start)) != std::string::npos) &#123; if (end != start) &#123; tokens.push_back(path.substr(0, end)); &#125; start = end + 1; &#125; if (end != start) &#123; tokens.push_back(path); &#125; vector&lt;string&gt;::const_iterator itp = tokens.begin(); while (itp != tokens.end()) &#123; if (access(itp-&gt;c_str(), 0) == -1) // 判断文件夹是否存在 &#123; cout&lt;&lt;*itp&lt;&lt;" is not existing"&lt;&lt;endl; cout&lt;&lt;"now make it"&lt;&lt;endl; if (mkdir(itp-&gt;c_str()) == 0) // 不存在则创建，只能一级一级的创建 &#123; cout&lt;&lt;"make successfully"&lt;&lt;endl; &#125; &#125; cout &lt;&lt; *itp++ &lt;&lt;endl; &#125;&#125;// Txt文件转opencv Mat（txt文件中存的是以行列形式的深度值）cv::Mat Txt2DepthMat(const string &amp;txtname)&#123; cv::Mat result(480, 640, CV_8UC1, cv::Scalar(0)); char buffer[12800]; // 按行读取文件 fstream outFile; const char *d = ","; // 以,为分割点 char *p; // 分割出的子串 outFile.open(txtname, ios::in); for (int i = 0; outFile.getline(buffer, 12800, '\n') != NULL &amp;&amp; i &lt; result.rows; i++) &#123; p = strtok(buffer, d); for (int j = 0; p &amp;&amp; j &lt; result.cols; j++) &#123; int realDepth = (atoi(p) &amp; 0xfff8) &gt;&gt; 3; //提取距离信息，高13位 int depth = (int)(256 * realDepth / 0x0fff); //因为提取的信息是距离信息，为了便于显示，这里归一化为0-255 result.at&lt;uchar&gt;(i, j) = cv::saturate_cast&lt;uchar&gt;(depth); p = strtok(NULL, d); &#125; &#125; outFile.close(); return result;&#125;// 以颜色表示深度信息，越暖（红色）越近，越冷（蓝色）越远cv::Mat Depth2Color(const cv::Mat &amp;depth)&#123; cv::Mat result(depth.size(), CV_8UC3, cv::Scalar::all(0)); int tempDepth, depthRed, depthGreen, depthBlue; for (int i = 0; i &lt; result.rows; i++) &#123; for (int j = 0; j &lt; result.cols; j++) &#123; tempDepth = 255 - depth.at&lt;uchar&gt;(i, j); if(tempDepth &lt; 43) &#123; depthRed = tempDepth * 6; depthGreen = 0; depthBlue = tempDepth * 6; &#125; if(tempDepth &gt; 42 &amp;&amp; tempDepth &lt; 85) &#123; depthRed = 255 - (tempDepth - 43) * 6; depthGreen = 0; depthBlue = 255; &#125; if(tempDepth &gt; 84 &amp;&amp; tempDepth &lt; 128) &#123; depthRed = 0; depthGreen = (tempDepth - 85) * 6; depthBlue = 255; &#125; if(tempDepth &gt; 127 &amp;&amp; tempDepth &lt; 169) &#123; depthRed = 0; depthGreen = 255; depthBlue = 255 - (tempDepth - 128) * 6; &#125; if(tempDepth &gt; 168 &amp;&amp; tempDepth &lt; 212) &#123; depthRed = (tempDepth - 169) * 6; depthGreen = 255; depthBlue = 0; &#125; if(tempDepth &gt; 211 &amp;&amp; tempDepth &lt; 254) &#123; depthRed = 255; depthGreen = 255 - (tempDepth - 212) * 6; depthBlue = 0; &#125; if(tempDepth &gt; 253) &#123; depthRed = 255; depthGreen = 0; depthBlue = 0; &#125; if (tempDepth == 255) &#123; depthRed = 0; depthGreen = 0; depthBlue = 0; &#125; result.at&lt;Vec3b&gt;(i, j)[0] = depthBlue; result.at&lt;Vec3b&gt;(i, j)[1] = depthGreen; result.at&lt;Vec3b&gt;(i, j)[2] = depthRed; &#125; &#125; return result;&#125;int main(int argc, char *argv[])&#123; string filePath = "C:/Users/XXXXXX/Downloads/NTU-Microsoft-Kinect-HandGesture Dataset/Depth"; vector&lt;string&gt; files; //读取所有文件 string format = "*"; // 不知道为什么在我电脑读不了特定文件？ getAllFormatFiles(filePath, format, files); for (int i = 0; i &lt; files.size(); i++) &#123; cv::Mat tempMat = Txt2DepthMat(files[i]); files[i].replace(0, 66, "../data"); files[i].replace(files[i].find(".txt"), files[i].length() - 1, ".png"); cout&lt;&lt; files[i] &lt;&lt; endl; string tempString = files[i].substr(0, files[i].find_last_of("/")); makeDir(tempString); cv::imwrite(files[i], tempMat); &#125; cout &lt;&lt; "File Size: " &lt;&lt; files.size() &lt;&lt; endl; //cv::imshow("test", Depth2Color(Txt2DepthMat("./1.txt"))); cv::waitKey(0); return 0;&#125; 后记 正如前言所说，本文是以前记录过的，一些细节也快忘记，这次重写算是回顾一下吧，这段程序可能也确实只用这么一次，但其中用到了不少C++处理字符串和读写文件等相关知识，而这些知识，在以后有极大的可能会再次用到，因此记录↖(&#94;ω^)↗。 参考资料[1] C++文件读写操作（二）逐字符读取文本和逐行读取文本（http://blog.csdn.net/shihui512/article/category/1397194 ） [2] 字符串分割(C++)（http://www.cnblogs.com/MikeZhang/category/345894.html ） [3] C++读取文件夹中所有的文件或者是特定后缀的文件（http://blog.csdn.net/adong76/article/category/1632029 ） [4] C/C++ 判断文件夹是否存在以及创建、删除文件夹 windows以及linux通用（http://blog.csdn.net/u012005313/article/category/5586103 ） [5] Split a string in C++?（http://stackoverflow.com/questions/236129/split-a-string-in-c ） [6] Kinect开发学习笔记之（六）带游戏者ID的深度数据的提取（http://blog.csdn.net/zouxy09/article/category/1273380 ） [7] Depth Map Tutorial（http://www.pages.drexel.edu/~nk752/depthMapTut.html ）]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>c/cpp</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV中滑动条和鼠标事件响应操作的使用小结]]></title>
    <url>%2F2017%2F10%2F08%2FOpenCV%E4%B8%AD%E6%BB%91%E5%8A%A8%E6%9D%A1%E5%92%8C%E9%BC%A0%E6%A0%87%E4%BA%8B%E4%BB%B6%E5%93%8D%E5%BA%94%E6%93%8D%E4%BD%9C%E7%9A%84%E4%BD%BF%E7%94%A8%E5%B0%8F%E7%BB%93%2F</url>
    <content type="text"><![CDATA[前言 既然在上文中提到了回调函数，本人就干脆把OpenCV中较常使用的两个使用回调函数的函数使用方法也一并记录下来吧。 说明篇OpenCV中使用回调函数的两个函数为： 鼠标事件响应操作函数：void cv::setMouseCallback(const string&amp; winname, MouseCallback onMouse, void* userdata = 0); 参数浅解： const string&amp; winname：窗口名称，对名为winname的窗口执行鼠标事件响应操作； MouseCallback onMouse：鼠标响应事件回调函数，监听鼠标的点击，移动，松开，判断鼠标的操作类型并做出相应处理； void* userdata：对应回调函数的可选参数，若使用全局变量可以忽略该参数。 对应的回调函数声明为：typedef void (*MouseCallback)(int event, int x, int y, int flags, void* userdata); 参数浅解： int event：鼠标滑动（CV_EVENT_MOUSEMOVE）、左键单击（CV_EVENT_LBUTTONDOWN）、右键单击（CV_EVENT_RBUTTONDOWN ）等10种鼠标点击事件的int型代号； int x, int y：鼠标位于窗口的（x，y）坐标位置，窗口左上角默认为原点，向右为x正轴，向下为y正轴； int flags：鼠标左键拖拽（CV_EVENT_FLAG_LBUTTON）、右键拖拽（CV_EVENT_FLAG_RBUTTON）等6种鼠标拖拽事件的int型代号； void* userdata：回调函数的参数，若使用全局变量可以忽略该参数。 创建滑动条函数：int cv::createTrackbar(const string&amp; trackbarname, const string&amp; winname, int* value, int count, TrackbarCallback onChange=0, void* userdata=0); 参数浅解： const string&amp; trackbarname：创建的滑动条名称； const string&amp; winname：所在窗口名称，对名为winname的窗口添加滑动条； int* value：滑块的位置，其初始值对应滑块的初始位置； int count：滑块可达到的最大位置的值，滑块最小位置的值总为0； TrackbarCallback onChange：滑动条事件回调函数，当滑动条上位置改变的时，则执行该回调函数； void* userdata：对应回调函数的可选参数，若使用全局变量可以忽略该参数。 对应的回调函数声明为：typedef void (CV_CDECL *TrackbarCallback)(int pos, void* userdata); 参数浅解： int pos：滑动条的位置对应的值； void* userdata：回调函数的参数，若使用全局变量可以忽略该参数。 *注：本文的函数说明采用的是opencv-2.4.11的函数声明，与opencv-3.2.0的函数声明区别在于string类型，opencv-3.2.0采用的是其自己实现的一个String类。 实例篇Show u the code，具体C++实现代码为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142#include &lt;opencv2/opencv.hpp&gt; #include &lt;opencv2/core/core.hpp&gt;#include &lt;opencv2/imgproc/imgproc.hpp&gt;#include &lt;opencv2/highgui/highgui.hpp&gt;// ---------------- 鼠标事件回调函数 ---------------------------------static cv::Mat src_img; // 原始图像全局变量static void mouseCallback(int event, int x, int y, int flags, void *)&#123; bool selected = false; static cv::Point left_top_vertex, right_down_vertex; // 左上角顶点和右下角顶点 // When the left mouse button is pressed, record its position and save it in corner1 if (event == CV_EVENT_LBUTTONDOWN) // 左键按下 &#123; left_top_vertex.x = x; left_top_vertex.y = y; std::cout &lt;&lt; "Corner 1 recorded at " &lt;&lt; left_top_vertex &lt;&lt; std::endl; &#125; // When the left mouse button is released, record its position and save it in corner2 if (event == cv::EVENT_LBUTTONUP) // 左键弹起 &#123; // Also check if user selection is bigger than 20 pixels (jut for fun!) if (abs(x - left_top_vertex.x) &gt; 10 &amp;&amp; abs(y - left_top_vertex.y) &gt; 10) &#123; right_down_vertex.x = x; right_down_vertex.y = y; std::cout &lt;&lt; "Corner 2 recorded at " &lt;&lt; right_down_vertex &lt;&lt; std::endl &lt;&lt; std::endl; selected = true; &#125; else &#123; std::cout &lt;&lt; "Please select a bigger region" &lt;&lt; std::endl; &#125; &#125; // Update the box showing the selected region as the user drags the mouse if (flags == CV_EVENT_FLAG_LBUTTON) // 左键拖拽 &#123; cv::Point pt; pt.x = x; pt.y = y; cv::Mat local_img = src_img.clone(); rectangle(local_img, left_top_vertex, pt, cv::Scalar(0, 0, 255)); imshow("Cropping app", local_img); &#125; // Define ROI and crop it out when both corners have been selected if (selected) &#123; cv::Rect box; box.width = abs(left_top_vertex.x - right_down_vertex.x); box.height = abs(left_top_vertex.y - right_down_vertex.y); box.x = cv::min(left_top_vertex.x, right_down_vertex.x); box.y = cv::min(left_top_vertex.y, right_down_vertex.y); // Make an image out of just the selected ROI and display it in a new window cv::Mat crop(src_img, box); cv::namedWindow("Crop"); imshow("Crop", crop); &#125;&#125;// ---------- 响应鼠标事件 ------------------------------------void setMouseCallbackTest()&#123; src_img = cv::imread("../data/lena.jpg", CV_LOAD_IMAGE_ANYDEPTH | CV_LOAD_IMAGE_ANYCOLOR); cv::namedWindow("Cropping app"); imshow("Cropping app", src_img); // Set the mouse event callback function cv::setMouseCallback("Cropping app", mouseCallback); while (char(cv::waitKey(30)) != 'q') &#123;&#125;&#125;// -------------- 滑动条回调函数 ------------------------static void thresholdCallback(int slider_value, void* gray)&#123; //static_cast&lt;&gt;用于安全转换指针 cv::Mat *tmp_gray = static_cast&lt;cv::Mat *&gt;(gray); cv::Mat tmp = *tmp_gray; cv::Mat dst; threshold(tmp, dst, slider_value, 255, CV_THRESH_BINARY); //显示效果图 cv::imshow("Trackbar Demo", dst);&#125;// ------------ 创建滑动条 ----------------------------------void createTrackbarTest()&#123; cv::Mat src_gray = cv::imread("../data/lena.jpg", 0); const int max_value = 255; //滑动条的最大值 int slider_value = 0; // 滑动条的初始值 char *window_name = "Trackbar Demo"; char *trackbar_name = "Value:"; // 创建一个窗口显示图片 cv::namedWindow(window_name, CV_WINDOW_AUTOSIZE); imshow(window_name, src_gray); // 创建滑动条来控制阈值 createTrackbar(trackbar_name, window_name, &amp;slider_value, max_value, thresholdCallback, &amp;src_gray); while (char(cv::waitKey(30)) != 'q') &#123;&#125;&#125;// ------- 将两个函数在同一个窗口执行 ------------void callbackTest()&#123; src_img = cv::imread("../data/lena.jpg", 0); const int max_value = 255; //滑动条的最大值 int slider_value = 0; // 滑动条的初始值 char *window_name = "Callback Demo"; char *trackbar_name = "Value:"; // 创建一个窗口显示图片 cv::namedWindow(window_name, CV_WINDOW_AUTOSIZE); imshow(window_name, src_img); // 创建滑动条来控制阈值 createTrackbar(trackbar_name, window_name, &amp;slider_value, max_value, thresholdCallback, &amp;src_img); // 鼠标事件响应 cv::setMouseCallback(window_name, mouseCallback); while (char(cv::waitKey(30)) != 'q') &#123;&#125;&#125;int main(int argc, char *argv[])&#123; //setMouseCallbackTest(); //createTrackbarTest(); callbackTest(); while (char(cv::waitKey(30)) != 'q') &#123;&#125; return 0;&#125; 经本人测试，上面示例程序在opencv-2.4.11和opencv-3.2.0下都能完美运行。 后记 本来这两个函数都已经写（chao）好了，但为了更好的体现示例程序，又稍作了修改：添加鼠标左键拖拽事件及不使用全局变量等。 参考资料[1] opencv2 使用鼠标绘制矩形并截取和保存矩形区域图像（http://www.cnblogs.com/lidabo/category/516776.html ） [2] Opencv中添加进度条及回调函数（http://blog.csdn.net/weixin_35738542/article/category/6337413 ） [3] OpenCV2中滑动条（Trackbar）回调函数的小发现（http://blog.csdn.net/u014291399/article/category/3097955 ） [4] OpenCV GUI基本操作，回调函数，进度条，裁剪图像等（http://blog.csdn.net/wangyaninglm/article/category/1653815 ）]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用回调函数计算函数运行时间]]></title>
    <url>%2F2017%2F09%2F28%2F%E5%88%A9%E7%94%A8%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E5%87%BD%E6%95%B0%E8%BF%90%E8%A1%8C%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[前言 曾有一段时间在写一个小程序，由于其对运行时间有要求，所以每写一段代码就要测试一下运行时间，如果超出就需要优化一下代码或换一种方法和算法。但是每次都需要插在某两个位置插两段代码感觉有点烦，也有点浪费时间，毕竟浪费时间就是浪费生命，本着保尔柯察金关于生命的言论，本人不愿虚度年华，所以只得寻找一个方便简洁的方法计算运行时间（说了这么多，说到底其实就是懒吧*/ω\*）。后面就想到了回调函数，将想要计算运行时间的代码段放入一个函数中，并将其作为回调函数，用事先写好的计算时间函数调用它，从而方便计算该代码段的运行时间。 正文Show u the code，具体C++实现代码为： 12345678910111213141516171819202122232425262728#include &lt;ctime&gt;#include &lt;cstdio&gt;#define _CALLED_ printf("The function %s", __FUNCTION__);// 使用回调函数计算一段代码执行时间void computeTotalTime(void(*processingCallback)() = 0)&#123; clock_t start_time = clock(); processingCallback(); clock_t end_time = clock(); printf(" takes: %fs.\n", (double)(end_time - start_time) / CLOCKS_PER_SEC);&#125;void test()&#123; for (int i = 0; i &lt; 1000; i++) &#123; printf("Hello World!\n"); &#125; _CALLED_;&#125;int main(int argc, char *argv[])&#123; computeTotalTime(test); return 0;&#125; 后记 本来是想在网上找一个的，谁知道并没有找到，就只有自己动手实现一个了╮(╯_╰)╭。后面使用了一下该函数，发现好像并没有提高生产力o(╯□╰)o，所以就没人放在网上？-_-!，不过确实从实现过程中学到了一些东西↖(&#94;ω^)↗。 参考资料[1] C/C++之回调函数（http://www.cnblogs.com/danshui/category/345046.html ） [2] c/c++在windows下获取时间和计算时间差的几种方法总结（http://blog.csdn.net/coder_xia/article/category/837943 ） [3] (转)用宏获取函数名（http://www.cnblogs.com/steady/category/264974.html ）]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>c/cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论如何科学的上网]]></title>
    <url>%2F2017%2F09%2F22%2F%E8%AE%BA%E5%A6%82%E4%BD%95%E7%A7%91%E5%AD%A6%E7%9A%84%E4%B8%8A%E7%BD%91%2F</url>
    <content type="text"><![CDATA[科学式上网推荐组合：Chrome，Proxy SwitchyOmega，Lantern等代理工具。 前言 所谓的科学式上网，懂的自然懂，本人也就不做过多解释了。本来一直在用别人免费提供的pac代理，但最近可能别人关掉了，上不了google了，就只能另寻他路了。所谓的另寻他路也就是尝试云端框架网站站长枂下提供的另外几种科学式上网攻略。本文只是对枂下站长的攻略做一下实验记录，若想看原滋原味的攻略，还请移步云端框架。 科学的上网方法 尽量使用Chrome进行科学式上网，因为其有一个代理管理插件Proxy SwitchyOmega，该插件称之为代理切换神器也不为过，网上大量的教程和配置文件也是基于该神器做的。使用Proxy SwitchyOmega需要进行配置，这对初学者有一定的难度，这里本人推荐直接使用站长枂下提供的配置文件，至于SwitchyOmega的配置文件可以去站长的云端框架网站上去下，也可以联系本人。至于代理工具请看下文，本人目前也只尝试过使用以下几种工具。 lantern 其实本人最先尝试的工具是XX-NET，但是其配置起来稍显繁琐，而且在第一步的时候必须处在科学式上网环境，而Lantern就比较简单了，只要装上之后再稍微动动手脚就可以了，所以就把lantern写在第一位了。从站长枂下那下载【蓝】灯电脑破解版压缩包，不过本人觉得应该随便在哪里下载个原版lantern-2.2.5安装都可以，只要后续的破解方法一样即可。 具体破解方法为：主要是令lantern一直保持在2.2.5版本不变。但是一般来说lantern在安装之后会自动更新到最新版（本人在两台电脑上都安装过lantern，其中一台安装完之后打开lantern安装文件夹发现其已更新，而另一台却没有更新，这就有点玄学了-_-!），至于判断lantern有没有更新的办法是：首先进入lantern的安装文件夹：C:\Users\XXX\AppData\Roaming\Lantern（将XXX改成自己的用户名），1、看lantern.exe文件的修改日期，如果还是2016年的，就说明其还没更新；2、显示隐藏文件，看有没有.lantern.exe.old文件，如果没有，则也还没更新。如果已经更新了，参考站长枂下的说法： 删除lantern.exe文件，修改.lantern.exe.old为lantern.exe 这样就又可以回退至lantern-2.2.5版。如果没更新的话就不用进行删除回退这一步，直接进行下一步。 下一步为修改lantern-2.2.5.yaml文件中的更新路径updateserverurl，使lantern永远不再更新，一直维持在2.2.5版本不变。具体更改方式为： 将其中的 updateserverurl: https://update.getlantern.org 修改为 updateserverurl: https://pic.black1ce.com 修改完之后保存退出。这里本人觉得可以随便将其修改成其它路径即可，毕竟只是让其不更新而已，这个路径应该除了更新就没有其它作用了，这纯属本人拙劣的猜测，有（ai）兴（gao）趣（shi）的童靴可以试试:-P。 原本以为到这一步就完成了，但是枂下站长后来又补发了一步，就是上面几步只是让lantern不再更新，而500M流量之后限速的问题仍然存在（本人目前还没超过500M，所以不知道这个问题，但抱着有备无患的心态先把枂下站长的攻略记一下O(∩_∩)O~），所以接下来才是上正菜，破解“限制500M流量”问题的具体方法为：当使用lantern流量超过500M时，打开lantern的安装目录，打开lantern-2.2.5.yaml文件， 修改其中第九行的设备号，随意更换一个数字或者字母即可。 按枂下站长的说法是8位随机字母数字大小写均可，只是为方便起见推荐只改动某位即可。 eg：本人目前第9行为：deviceid: Gu25Sfoz，一旦500M流量用完了，本人就只需要将其修改为deviceid: Gu25Sfoa即可。 到这一步lantern的破解算是基本完成了吧，如果枂下站长有新的更新且被本人看到的话再进行实验更新吧。 XX-NET 该工具应该是本人尝试配置的首款代理工具，不得不说其配置和lantern相比实在是太复杂了，而且其中有一步还必须处在科学式网络环境中，本人还是借助别人的VPN上的（当时还没用lantern，所以没用其500M免费不限速的流量~~o(&gt;_&lt;)o ~~）。本人经过实测XX-NET无法在Firefox中用google搜索，一用google搜就会报错： 您的连接并不安全 www.google.com 的网站管理员未正确配置网站。为避免您的信息被窃，Firefox 没有与该网站建立连接。 此网站采用了 HTTP 严格传输安全（HSTS）机制，要求 Firefox 只能与其建立安全连接。正因如此，您也不能将此证书加入例外列表。 www.google.com 使用了无效的安全证书。 该证书因为其颁发者证书未知而不被信任。 该服务器可能未发送相应的中间证书。 可能需要导入一个额外的根证书。 错误代码: SEC_ERROR_UNKNOWN_ISSUER https://www.google.com/search?q=test&amp;ie=utf-8&amp;oe=utf-8 对等端的证书颁发者不受认可。 HTTP 严格传输安全（HSTS）：falseHTTP 公钥钉扎：true 证书链： —–BEGIN CERTIFICATE—–MIIDkzCCAnugAwIBAgIQSu4RvcIwnqiEQE6Z68FlaDANBgkqhkiG9w0BAQsFADBzMQswCQYDVQQGEwJDTjERMA8GA1UECAwISW50ZXJuZXQxDzANBgNVBAcMBkNlcm5ldDEQMA4GA1UECgwHR29BZ2VudDEVMBMGA1UECwwMR29BZ2VudCBSb290MRcwFQYDVQQDDA5Hb0FnZW50IFhYLU5ldDAeFw0xNzA5MTYxNDIzMjRaFw0yNzA5MTQxNDMzMjRaMHgxCzAJBgNVBAYTAkNOMREwDwYDVQQIDAhJbnRlcm5ldDEPMA0GA1UEBwwGQ2VybmV0MRcwFQYDVQQLDA5Hb0FnZW50IEJyYW5jaDEVMBMGA1UEAwwMKi5nb29nbGUuY29tMRUwEwYDVQQKDAwqLmdvb2dsZS5jb20wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDL3K1OgwalKOJPtO4urpAiu+lioGNax/EIaYR1D2kH66AJlpal0pYFhXF6MOYCUNfpZIqP5qAQs7JGuRmFdo7rWaLHZ+3S+TlIHdZkoLvyYBcXENVBcLQvZ7IL7DDUZObK/R7OOKz82dEoITQnT+q/lecR9wQ7QNdNVNqn0xS0NPt7bS76irMxkJcO2q7Lu4R56ImCox/G7dUEepjL0Po516l6fLKG3qi5org2z6ap0yl2Etu8cRfqiqaqhO0HI1Twz+Rbp/8KUdUBgnNkjcod83HE+jJKxIUDmn18+l7J8sBia0JvWSIYy2ccFXoR8L4lfvIa8PhTuMmpxyDkwDdPAgMBAAGjHjAcMBoGA1UdEQEB/wQQMA6CDCouZ29vZ2xlLmNvbTANBgkqhkiG9w0BAQsFAAOCAQEADiM6yWCaGNLnggirjN0b34j5JmjgYYx3bRaKDe4We2emjlLsdskBo2ztkd/tPBfUa7DWExgFPvVqB2FeEf85Zj72kMmc2JikJBtPF1qK9fa4O1gST4VE0xIF99zGrgkDhGaYd1ocElWSqfBNQfzwsO+nl2OQf99ATMqMSCGacN7z+LJBLn65de+ODzYUkIHzhU5/xJMian3yfQzNFCAgK8OMf16excqRUcX8zfGPfvtAafDrdOYEXcGayLIvt4tGr8T+tii+MtCRO5hXK8/ABMLGI74zgLYloVFjJv21VsLrNCvvD43T5E3c+8d1MENozdEnsyzWkTkpknP4aEiLOQ==—–END CERTIFICATE—– 本人不知道为什么(+﹏+)~。所以为了能正常使用科学式网络，还是老老实实的用Chrome吧，何况其还有Switchyomega神器，并且建议把Chrome设置为默认浏览器。具体配置流程如下： 从枂下站长那下载XX-NET（本人其实最先是直接在GitHub上下载最新的XX-NET，但本人由于在Firefox上尝试失败了，当时也没在Chrome上尝试，以为最新版XX-NET有问题，后来用上枂下站长那下的XX-NET在Chrome上试了下可以，而Firefox不行，就知道可能是Firefox的问题，而既然已经能用了，本人也没用最新的XX-NET在Chrome上尝试了），直接解压到某个文件夹，然后将XX-Net-3.3.6文件夹重命名为XX-Net，即去掉末尾的版本号，据枂下站长说法是为了减少后续XX-NET出错（本人这里老老实实的照枂下站长说的做了，所以也不知道如果没去掉版本号会有什么后果）。接着以管理员身份运行XX-Net目录下的start.vbs文件（这里右键是没有“以管理员身份运行”选项的，要想以管理员身份运行就只有使用Windows命令行了，具体做法就是以管理员身份运行“命令提示符”，再在其中运行start.vbs文件即可），运行成功后将弹出 已经导入GoAgent证书,请重启浏览器. 点击确定即可。再次启动默认（Chrome）浏览器，将打开127.0.0.1:8085页面，即为XX-NET的配置界面。将看到GAEProxy状态信息（可能是由于google取消了公共APPID，所以本人看到的不是“您正在使用公共APPID，….”这条消息，而是另外一条消息（具体什么消息本人忘记了o(╯□╰)o）），打开显示详细信息（其实也没用，本人并看不懂这么多-_-|||），先放这里吧，部分信息以后再说，先进入正式配置步骤。 首先点击左边的“高级”选项，据枂下站长说，将自动调整扫描线程数关掉，最大扫描线程数设为200，点击提交（可能这样连接速度更快一些）。本人这里这里没有照做，而是保持默认设置，本人只是要求能上就可以了，对速度要求可以稍微放松一点（或许以后会调成站长推荐的配置）。 接下来点击“部署服务端”选项，填写AppID，点击“开始部署”。但是这一步，本人并没有AppID，所以只能上google申请，这样最麻烦的一步就来了。照着枂下站长的指引，本人一步步的申请了AppID。具体申请步骤如下（这一步需要登录google账号，必须处于科学式网络环境）： 点击打开Appid申请页面，登录google账号，创建项目，并在新建项目中修改项目ID（这就是第一个AppID），为方便，建议直接以“项目名-00”的方式按顺序命名AppID，创建成功后继续点击“Google Cloud Platform”旁的一个小三角，点击弹框的“+”，按上述方式进行创建新的AppID，如此重复，本人总共创建10个AppID，10个之后会提示配额已用完，硬是要创建只会覆盖掉第一个AppID。 接下来就是需要选择语言和地区，只有为每个AppID选择语言和地区之后，该AppID才会生效。语言和地区的选择界面可以从“App Engine”界面中进入，也可以直接在添加AppID界面的那个弹框中点击相应的AppID进入，当然如果是第一个AppID选择语言可以直接点击界面上的“选择一种语言” 。语言选择Python，地区选择亚洲（asia-northeast1），选错了后果自负（当然可以覆盖掉该AppID重新设置）。其实可以照枂下站长那样设置完第一个AppID的语言和地区之后直接修改浏览器的地址栏的url以快速设置AppID的语言和地区。具体修改方法为：将url地址末尾的project参数的值改为你想要设置的下一个AppID，eg:本人当前AppID为test-00，对应的浏览器url地址末尾的参数为lang=python&amp;project=test-00，本人想设置的下一个AppID为test-01，则只需将其修改为lang=python&amp;project=test-01回车即可快速设置test-01的语言和地区。如此重复，就可以设置完全部的AppID语言和地区。设置完之后，这些AppID就能进行部署了（据说每个AppID每天有1G的流量可以使用，并于每天下午三点更新，也就是本人每天有10G流量，一般是够用了:-D）。 将上文申请并设置完成的AppID放入“GAE AppID”文本框中，多个AppID可以按这样的格式放入：test-01|test-02|test-03，两个AppID以|分隔即可。点击“开始部署”，会弹出一个登录google账号的标签页，登录并允许即可，等待2~5分钟，可发现日志页面出现Done!和Deploy 10 appid successed.等字样即表示服务端部署成功。 点击“部署”选项，将上面部署服务端的AppID以相同格式输入“GAE AppID”文本框中，点击“保存”即可。保存完之后，即可在状态信息界面显示详细信息中Appid发现当前工作AppID就是部署的AppID，至于那个配置下的监听代理就是设置代理的地址和端口。 这样XX-NET就算是配置完成了，至于枂下站长说的扫描ip，本人就没做实验了，因为这样配置完就可以科学式上网了。 以上代理工具配置完成后，即可在Chrome中畅游Internet了，但是正确的科学式上网姿势应该是：国内的网站走本地连接，而国外被屏蔽的网站才走代理。这就需要Proxy SwitchyOmega这款插件了，它能按照一定的规则自动选择走本地还是走代理，这样既不会浪费流量，也能使国内的网站联网速度不受影响。导入前文推荐的配置文件后，就可选择对应的代理方式。这里当然是选择自动切换，至于虚拟切换是选择Lantern for 8787还是XXNET for GAE就随便个人的喜好了（在走代理的时候别忘了把相应的代理工具开启）；如果直接选择其中一种代理方式就相当于全局代理，这也就失去这款插件的作用，只有自动切换加上虚拟切换才能充分发挥这款神器的真正作用。 匿名网络 要想使用匿名网络，当然少不了专用的浏览器：Tor Browser，下载并安装（下载时需要身处科学式网络环境，安装时最好改变一下目录，而且路径中最好不要有中文）。接下来就是配置了Tor网络了。具体配置流程如下： 首先，它问直接连接Tor网络还是配置网桥或代理，这里当然是选择配置；其次它问互联网服务提供商(ISP)是否对Tor网络连接进行了封锁或审查，这里选否，据枂下站长所说因为国内网桥大部分已失效，连接网桥没有意义还会拖慢速度；然后它问是否本地代理访问互联网，这里当然选择是；最后填写本地代理配置，这里需要注意，枂下站长提供的部分代理配置是： SSR/SS Socks5//127.0.0.1 : 1080 Seed HTTP//127.0.0.1 : 1080 Lantern Socks5//127.0.0.1 : 8287（2系列），三系列的在Lantern设置页面查看 Psiphon 可以在配置页面自定义 其中经本人实测，上面Lantern的代理配置是连接不上的，本人后来参考SwitchyOmega配置文件中Lantern的代理为HTTP//127.0.0.1 : 8787，经尝试如此配置可以连接Tor网络，所以Lantern的正确配置应为： Lantern HTTP//127.0.0.1 : 8787（2系列），三系列的在Lantern设置页面查看 设置完成后等待片刻就能连上Tor网络了，最好就保持原来的DuckDuckGo搜索引擎，不要更改，接下来就可随心所欲的畅游Internet了。 至于想访问暗网，可以参考Hacking/整理的暗网网址Tor.txt和Working Links to the Deep Web或者直接用站长枂下给的网址：torlinkbgs6aabns.onion和xmh57jrzrnw6insl.onion。 据枂下站长回答： XX-Net可以作tor的前置代理吗？不行的，xx-net是假http协议 所以XX-NET不能用作Tor的代理配置。 最后再简要记录一下Chrome调用Tor Browser的代理吧。本人没有像枂下站长那样用命令行去实验，只是享受了一下ta的试验成果（O(∩_∩)O谢谢）。总而言之，还是利用SwitchyOmega，代理方式选择Tor for 9150，就可以在Chrome中调用Tor Browser的代理，畅游Internet了。 后记 最后的匿名网络是本人弄着好玩的，像暗网这种东西本人这种遵纪守法的好公民才不会访问呢(ಡωಡ)。等以后时机到了再去买个国外VPS自己搭建一个科学式上网环境吧。最后感谢枂下站长的无私分享。 附录 原本还以为本人搭建的Hexo+GitHub个人博客站点还是个深网，没想到搞完科学式上网后用google搜索竟然能搜到，虽然本人没做什么，但google仍然能搜到，google的蜘蛛还挺厉害的，不过如果百度的蜘蛛没被GitHub屏蔽的话百度可能也能搜到（从某些原因上来说，GitHub把百度屏蔽掉也好O(∩_∩)O~）。既然已经被google收录了，本人也就不去搞那个站点地图了，等以后想搞SEO了再去做吧。 参考资料[1] Switchyomega超详细教程之Chrome与Firefox版本 [2] 【蓝】灯电脑破解版之2系列禁止自动升级最终办法 [3] XX-NET史上最详细完整教程 [4] XX-NET史上最详细完整教程之第一部分：Appid创建部分 [5] Tor Browser在国内Windows平台下的超详细教程 [6] Chrome等其他程序如何完美调用Tor Browser的代理来上网]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>gfw</tag>
        <tag>record</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo的SPFK主题修改小记]]></title>
    <url>%2F2017%2F09%2F16%2FHexo%E7%9A%84SPFK%E4%B8%BB%E9%A2%98%E4%BF%AE%E6%94%B9%E5%B0%8F%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言 本人一直在对Hexo的SPFK主题进行持续修改以符合本人自己的需求，在修改当中也会遇到一些小问题，以防遇到重复问题，特此记录所遇小问题，至于大问题可能会另外开篇。 修改篇1、修改aboutme排版问题需求描述：本人为了使aboutme排版好看一点，使“关于我”的内容更有段落感，本人尝试在主题配置文件中aboutme对象的内容添加各种换行转义符号均于事无补，如\n、\r\n、&amp;#13;、&amp;#10;、&lt;br /&gt;等，站点不仅不会换行，还会直接将转义符号都显示出来(╯﹏╰）。 解决办法：既然本人基本把所有的换行方法都试过了，还没有任何作用，那就只能是问题出在其它地方了。本人首先找到显示aboutme内容的地方，其位于主题文件夹下\layout\_partial\left-col.ejs，显示aboutme内容的代码为&lt;div id=&quot;js-aboutme&quot;&gt;&lt;%=theme.aboutme%&gt;&lt;/div&gt;，查阅相关资料，具体为与大家分享ejs源码阅读心得，其中有这样一段话： 关于ejs模板的五种模式对应几种指令 ejs主要提供了如下几种指令: &lt;%, 该指令主要通过js中的eval来执行js代码, 如上模板代码&lt;% [1,2].forEach(function(v){ %&gt;将通过eval编译成; [1,2].forEach(function(v){即直接可执行的js代码, 并且不会存放到__output函数中输出. &lt;%=, 该指令主要用于输出变量内容, 如上模板代码&lt;%= v %&gt;将通过escape函数编译成__append(escape(v)), 可以看到该指令用于输出变量内容, 最后将通过__output输出内容. &lt;%-, 该指令与&lt;%=区别是, &lt;%=指令使用escape函数来对特殊字符进行编码, 如将&gt;转为%3E, 查看关于escape函数. &lt;%#, 该指令主要用于模板内注释, 既不会执行也不会输出. &lt;%%, 主要用于输出字面值%. 关于以上各个指令对应的解析, 可参考ejs源码根目录lib/ejs.js文件中的scanLine函数. 从中可得知&lt;%=指令会将变量内容中一些特殊字符先转义，再原封不动的输出，所以本人无论怎么修改主题配置文件中aboutme对象的值，其输出内容都会是原封不动的aboutme对象的值。为了让其输出内容可以有相应的特殊格式，就不能让其转义，只能用&lt;%-指令，将其修改为&lt;div id=&quot;js-aboutme&quot;&gt;&lt;%-theme.aboutme%&gt;&lt;/div&gt;，这样就能使输出内容可以自定义特殊格式，本人最后在aboutme对象的内容中需要换行的地方添加了&lt;br /&gt;，实测如此修改后可以换行。 2、给左栏添加滚动条需求描述：SPFK主题是双栏的主题。因为左栏主要是用来显示一些菜单和头像等内容，这些内容也不多，所以原作者就没有添加滚动条。但是由于本人添加了个本地搜索功能，在刚开始文章少的时候还不受影响，但是随着文章的增多，搜索功能就会影响左栏的布局，这是就必须添加一个滚动条了。本以为添加滚动条很简单，就是添加一个overflow: auto;，谁知道还没这么简单╮(╯﹏╰）╭。 解决办法：本人对问题的定位没问题，就是修改主题文件夹下的\source\css\_partial\main.styl文件中.left-col样式，问题在于怎么修改，本想直接在其中加入overflow: auto;，按道理说问题就能解决的，但是本人去搜索试试，发现搜索框上方的头像，文字等全部消失了，滚动条没起到作用，而下方的菜单可以通过滚动条看到。于是本人觉得可能是div上界没撑开，而超出的地方却隐藏了，但下界为什么能撑开，本人这里还是很不明白?_?。既然是这里隐藏了，本人就去看相关标签有没有overflow: hidden;属性，谁知道要么是没有，要么是即是关闭了也没有作用，那问题应该不是出在这里。就只能是这些元素所在的子div里了，本人找到其子div属性.intrude-less，其中虽有overflow: auto;但没设置height属性，所以就不能发挥其作用，本人于是给它加上height属性，搜索后发现有两个滚动条，这显然不简约，于是本人把.intrude-less的overflow: auto;属性注释掉，没想到居然能完美解决问题，可能是因为加上高度属性之后就能撑大父元素div了吧（来自某业余前端的猜测(⊙_⊙)）。后面为了更美观，本人把下方菜单区域的div样式.switch-area高度min-height改小了一点，顺便也把主题文件夹下的\layout\_partial\left-col.ejs文件中首行注释掉&lt;!-- &lt;div class=&quot;overlay&quot;&gt;&lt;/div&gt; --&gt;。本人也曾想把height改为min-height，谁知道又出现相同的问题，不得不又改回去。虽然这次已经解决了问题，但有些细节问题还是不太明白，只有等以后前端水平上去了再去想了，如有大佬知道还望不吝赐教(&#94;人&#94;)。 3、修改打赏问题问题描述：本人突然想玩一下那个打赏小东西，但照配置文件中指示的那样在文章开头ymal格式中加入reward: true属性，没有任何作用，于是去主题文件夹搜索reward属性相应的代码，结果是“找不到结果”（坑爹了这是，摔！（╯‵□′）╯︵┴─┴ ）。 解决办法：既然reward属性找不到就只有搜索reward_type属性，最终在主题文件夹下\spfk_c\layout\_partial\article.ejs文件中找到这样一条语句&lt;% if ((theme.reward_type === 2 || (theme.reward_type === 1 &amp;&amp; post.toc)) &amp;&amp; !index){ %&gt;，其下面就是打赏相关的代码，查看SPFK主题原作者介绍信息（Hexo 主题：SPFK）发现toc属性是用来显示目录的（一个用来打赏的代码怎么与文章目录相关了-_-#），所以上面的toc应该改成reward，修改后的代码为&lt;% if ((theme.reward_type === 2 || (theme.reward_type === 1 &amp;&amp; post.reward)) &amp;&amp; !index){ %&gt;，这时照配置文件中指示的那样在文章开头ymal格式中加入reward: true属性就能在相应的文章后面看到一个大大的“赏”字。 本来写到这里应该打赏这玩意应该完结了，但本人无意中在该文件的下面发现这样一段代码： 123&lt;% if (!index &amp;&amp; post.toc != false &amp;&amp; !is_page())&#123; %&gt; &lt;%- partial(&apos;_partial/toc&apos;) %&gt;&lt;% &#125; %&gt; 这是和toc（即文章目录）真正相关的代码，功能大概就是判断是否加载文章目录相关的代码，如果在文章开头设置toc: false，则该文章不会显示目录，但是如果在文章中不加toc属性，也会显示文章目录，但上面的打赏却不会显示，看起来post.toc != false和post.toc应该逻辑差不多，这里是本人感到十分奇怪的一个地方？后面查阅相关资料（JavaScript undefined 属性）得知: 注释：null 表示无值，而 undefined 表示一个未声明的变量，或已声明但没有赋值的变量，或一个并不存在的对象属性。 而本文这里因为没有在文章开头设置toc属性，所以其为undefined，其既不为false也不为true，只为undefined，当在if语句中做判断，会执行else分支，作!运算，结果则为：true。所以if(post.toc)不能执行其下代码，因为post.toc为undefined，不为true也不为false，而if(post.toc != false)能执行其下代码，因为post.toc != false为真。至于javascript中if(a == ture)和if(a)的区别具体为：前一种是a必须为1或者true才执行；而后一种只要a不为false undefined null 0 -0 NaN &quot;&quot;这7个中的其中任何一个都能执行。 待续。。。 后记 先就写到这里，如后续修改中发现问题再继续记录吧↖(&#94;ω&#94;)↗。]]></content>
      <categories>
        <category>建站小记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决Qt中Qlabel显示OpenCV的Mat数据图像产生扭曲现象问题]]></title>
    <url>%2F2017%2F09%2F16%2F%E8%A7%A3%E5%86%B3Qt%E4%B8%ADQlabel%E6%98%BE%E7%A4%BAOpenCV%E7%9A%84Mat%E6%95%B0%E6%8D%AE%E5%9B%BE%E5%83%8F%E4%BA%A7%E7%94%9F%E6%89%AD%E6%9B%B2%E7%8E%B0%E8%B1%A1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[前言 曾写过一个程序，需要有一个界面，但本人不想使用MFC，因缘巧合，在网上看到Qt，就尝试用了一下，遂有此文。本人的Qt版本为qt-opensource-windows-x86-msvc2013-5.6.2，看其名字就知道该版本的Qt可以通过Visual Studio 2013开发Qt程序（各位看官猜的没错，本人并没有直接使用Qt Creator开发Qt程序，而是通过VS开发Qt程序的\(&#94;o&#94;)/），一来是熟悉VS开发，对Qt Creator完全没用过；二来是已经在VS配好全套的开发环境了（画外音：说白了就是懒嘛╭(╯^╰)╮）。但是在VS中开发Qt程序还需要一些其它的配置。 准备篇 在VS中开发Qt程序首先需要安装一个addin外接程序，下载并安装qt-vs-addin-1.2.5.exe（http://download.qt.io/archive/vsaddin/ ），（网上说该程序已不支持VS2013及以上版本的VS，原因是VS2013及其以上版本的VS都不支持该种类型的插件，新版本的VS需要安装新型插件qt-vs-tools-msvc2013-2.1.1.vsix或 qt-vs-tools-msvc2015-2.1.1.vsix），但是经本人实测，本人的VS2013-update5 英文旗舰版通过qt-vs-addin-1.2.5编写Qt程序完全没问题，不过VS2015就不知道了，可能真需要安装新型插件。下载安装好相应的软件之后需要在VS中配置Qt环境，虽然不配置也能正常编译，但是会在Qt相关的语句下面出现红色波浪线，本人轻微强迫症表示不能忍╭(╯^╰)╮。具体配置如下： 选中“VC++目录”，在“包含目录”中添加： C:\Qt\Qt5.6.2\5.6\msvc2013\include 在“库目录”中添加： C:\Qt\Qt5.6.2\5.6\msvc2013\lib 配置完成之后即可发现红色波浪线已消失。 使用篇 VS中如何开发Qt程序请详见参考资料，懒癌发作，不想写了=_=（其实是因为要写的话只能贴图了，本人表示不想使用图片(╯﹏╰) ）。 问题篇问题描述：本人在用Qt显示OpenCV的Mat数据图像时，有时会发生扭曲现象（图像从对角线分开，两边颠倒，扭曲），有时却不会，为了撤了解决问题，查阅了相关资料，终于发现症结所在，原来是图片数据格式不符合Qt的图片数据格式。 解决办法：正文来喽~(≧▽≦)/~，就不说废话了，“Talk is cheap. Show you the code”，具体完整正确显示代码为： 123456789101112131415161718192021222324void showMatWithQtQlabel(const cv::Mat &amp;img, QLabel *label)&#123; // [Qt中用QLabel显示OpenCV中Mat图像数据出现扭曲现象的解决](http://lovelittlebean.blog.163.com/blog/static/11658218620125208212189/) QImage q_img; if(img.channels() == 3) // RGB image &#123; q_img = QImage((const uchar*)(img.data), img.cols, img.rows, img.cols*img.channels(), QImage::Format_RGB888).rgbSwapped(); &#125;else if (img.channels() == 4) // RGBA image &#123; q_img = QImage((const uchar*)(img.data), img.cols, img.rows, img.cols*img.channels(), QImage::Format_RGB32); &#125;else // gray image &#123; q_img = QImage((const uchar*)(img.data), img.cols, img.rows, img.cols*img.channels(), QImage::Format_Indexed8); &#125; // -------------- 图片自适应label ------------------- QImage q_label_img = q_img.scaled(label-&gt;size(), Qt::IgnoreAspectRatio, Qt::SmoothTransformation); // 图片自适应label大小 label-&gt;setPixmap(QPixmap::fromImage(q_label_img)); // 将图片显示到label上 // -------------- label自适应图片 ------------------- /*label-&gt;setPixmap(QPixmap::fromImage(q_img)); // 显示在label中 label-&gt;resize(label-&gt;pixmap()-&gt;size()); // 改变label的尺寸以自适应图像 label-&gt;show(); */&#125; rgbSwapped()函数是为了使Qt中显示图形颜色更自然，因为OpenCV的Mat数据RGB图像是以BGR的顺序排列，而Qt中是以RGB的顺序排列，所以需要rgbSwapped()交换一下颜色通道排列顺序。 附录1、摄像头数据采集问题注意：如果是从摄像头实时采集显示图像，在显示时需先判断图像有没有数据 1234567if (image.data)&#123; // 执行显示操作 showMatWithQtQlabel(mat, ui.label); // 执行其它操作...&#125; 或 1234567if (!image.empty())&#123; // 执行显示操作 showMatWithQtQlabel(mat, ui.label); // 执行其它操作...&#125; 具体原因可参考本人的一篇文章解决OpenCV-2.4.11调用摄像头显示拍摄视频出错问题。 2、信号与槽的连接函数问题Qt4中信号与槽的连接函数语法为： 1connect(&amp;theTimer,SIGNAL(timeout()),this,SLOT(getFrame())); // 超时就去取下一帧 而Qt5中信号与槽的连接函数新语法为： 1connect(&amp;theTimer, &amp;QTimer::timeout, this, &amp;QtTest::getFrame); //超时就去取下一帧 推荐使用Qt5新语法，具体原因可参考qt5中信号和槽的新语法 。 个人粗浅理解：信号函数一般是Qt中控件的库函数，比如按钮控件QButton的QButton::clicked ()函数，定时器QTimer的QTimer::timeout ()等函数；而槽函数是响应函数，一般由用户自己编写，也可以使用Qt中库函数。 使用Qt中可能会遇到的一些错误请参考使用VS2010开发Qt程序的一点经验（http://www.cnblogs.com/csuftzzk/category/445772.html ）。 后记 本来其实就想把问题篇写出来的，毕竟主要就是想记录一下那个显示函数，但是感觉有点没头没尾，就把VS集成Qt开发环境也稍微写了一下，而使用篇确实是因为参考资料已经写的很详细了，所以就直接一笔带过了。 参考资料[1] QT +openCV 实现摄像头采集以及拍照功能（http://blog.csdn.net/llh318724/article/category/930663 ） [2] VS2010 + QT5.2+ QT-VS-Addin1.2.2开发环境配置（http://blog.csdn.net/qqmindyourwill/article/category/5990841 ） [3] Qt+OpenCV界面（http://blog.csdn.net/fm0517/article/category/1110960 ） [4] Qt中用QLabel显示OpenCV中Mat图像数据出现扭曲现象的解决（http://blog.csdn.net/loveaborn/article/category/1164072 ）]]></content>
      <categories>
        <category>Problems</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>qt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决OpenCV-2.4.11调用摄像头显示拍摄视频出错问题]]></title>
    <url>%2F2017%2F09%2F16%2F%E8%A7%A3%E5%86%B3OpenCV-2.4.11%E8%B0%83%E7%94%A8%E6%91%84%E5%83%8F%E5%A4%B4%E6%98%BE%E7%A4%BA%E6%8B%8D%E6%91%84%E8%A7%86%E9%A2%91%E5%87%BA%E9%94%99%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文所用的OpenCV版本为opencv-2.4.11，编程语言为C++。 前言 本文其实是以前在刚学OpenCV时遇到的一个问题，当时我的环境还是：Win7，VS2010，opencv-2.4.11。当初就记录了下来，现在再来重新梳理一下。 问题篇问题描述：使用OpenCV-2.4.11调用摄像头显示拍摄视频时报runtime error，控制台窗口出现OpenCV Error: Assertion failed (size.width&gt;0 &amp;&amp; size.height&gt;0) in cv::imshow, file ……..\opencv\modules\highgui\src\window.cpp, line 261。 解决办法：在显示图片时先判断是否有图像数据，如下： 1234if (!image.empty()) &#123; imshow("window", image);&#125; 或 1234if (image.data) &#123; imshow("window", image);&#125; 原因可能是：用imshow()显示图像时，其image必须有数据，如果它为空则程序会报错，而一般打开摄像头会有一定时间的延迟，这时程序已经启动，而摄像头由于启动延迟，不一定能及时获取图像，造成要显示的image为空，因此报错。个人粗浅理解，板砖轻拍⊙﹏⊙b。 而网上有人也认为： 我也是遇到这个问题，不过看到一个帖子写得不错（英文的），里面给出了一个可能的理由，就是我们用opencv打开视频的时候，会自动先监测摄像头有没有读到帧，如果没有，就会报错，然后再执行你的程序，加一个if判断就是跳过系统自己的判断，直接执行我们的程序。来自：https://zhidao.baidu.com/question/1831122325089024420.html 有人说的原因是在VideoCapture刚开始获取摄像头视频流的过程不返回信号，所以判断Mat是否为空，并不断循环去获取Mat。来自：http://www.cnblogs.com/tiny656/p/3538115.html 附最终完整示例程序： 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;opencv2/core/core.hpp&gt; #include &lt;opencv2/imgproc/imgproc.hpp&gt; #include &lt;opencv2/highgui/highgui.hpp&gt;// 调用摄像头void videoCaptureTest()&#123; //cv::VideoCapture cap(0); // 打开默认摄像头，参数0代表默认摄像头的ID cv::VideoCapture cap; cap.open(0); // 设置摄像头 cap.set(CV_CAP_PROP_FRAME_WIDTH,640); cap.set(CV_CAP_PROP_FRAME_HEIGHT,480); // 确认是否成功打开摄像头 if (!cap.isOpened()) &#123; printf("打开摄像头失败，退出！\n"); exit(-1); &#125; cv::namedWindow("Capture", CV_WINDOW_AUTOSIZE|CV_WINDOW_FREERATIO); while (1) &#123; cv::Mat frame; cap &gt;&gt; frame; // 获取帧 // 对摄像头获取的帧进行各种处理 if (!frame.empty()) // 最好加上该判断，并在该判断中对帧进行处理 &#123; cv::imshow("Capture", frame); &#125; if(cv::waitKey(30) &gt;= 0) break; // 每30ms取一帧 &#125;&#125;int main(int argc, char *argv[])&#123; videoCaptureTest(); return 0;&#125; 其实也可以通过在获取帧时，反复获取帧，直到取到的帧有数据为止，这样就不需要判断语句了，直接显示即可，具体代码如下： 123456do&#123; cap &gt;&gt; frame;&#125;while(frame.empty());cv::imshow("Capture", frame); 参考自：https://stackoverflow.com/a/9285151 。 后记 本文还是当初在国内某平台写博客时写的，但现在再回头看，又稍微有了点新的思路，温故确实能知新(*&#94;__&#94;*) 嘻嘻……。 参考资料[1] OpenCV2.3使用摄像头和视频（http://blog.sina.com.cn/s/articlelist_2749877462_3_1.html ） [2] OpenCV Error: Assertion failed (size.width&gt;0 &amp;&amp; size.height&gt;0) in cv::imshow, fi 这个问题怎么办？ [3] OpenCV打开摄像头出现运行错误OpenCV Error：Assertion failed (size.width&gt;0&amp;&amp;size.height&gt;0)in cv::imshow，…… （http://blog.csdn.net/czl389/article/category/6381887 ） [4] [OpenCV]获取摄像头视频（http://www.cnblogs.com/tiny656/category/550972.html ）]]></content>
      <categories>
        <category>Problems</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加各种小部件]]></title>
    <url>%2F2017%2F09%2F15%2FHexo%E6%B7%BB%E5%8A%A0%E5%90%84%E7%A7%8D%E5%B0%8F%E9%83%A8%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[前言 本人目前还在使用对Hexo的主题SPFK自行魔改的那个主题（所谓的魔改也就是对照着black-blue主题修改了部分CSS，然后又添加了一个站内搜索功能(&gt;&#94;ω&#94;&lt;)），主题SPFK主体的东西其实都没改变。现在正逐渐将其完善中，遂有此文。 添加QQ邮箱联系 进入QQ邮箱开放平台，点击“获取邮我按钮”，登录QQ之后继续点击该按钮，因为本人不需要其样式，只需要其链接即可，所以就默认样式，直接点击“获取代码”即可，本人默认的“HTML代码”为： 1&lt;a target="_blank" href="http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=qNvAyd3G0d3JxujOx9DFycHEhsvHxQ" style="text-decoration:none;"&gt;&lt;img src="http://rescdn.qqmail.com/zh_CN/htmledition/images/function/qm_open/ico_mailme_01.png"/&gt;&lt;/a&gt; 提取其中的href，即http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=qNvAyd3G0d3JxujOx9DFycHEhsvHxQ，将该链接添加到主题配置文件中，具体如下： 12subnav: mail: "http://mail.qq.com/cgi-bin/qm_share?t=qm_mailme&amp;email=qNvAyd3G0d3JxujOx9DFycHEhsvHxQ" 重新部署站点即可发现对应的邮箱图标，点击该图标可直接给本人发邮件。 添加QQ交谈链接 进入QQ推广，点击上方的“推广工具”，若没登录QQ则先登录QQ，组件样式同样默认即可，这里需要注意的是，需要点击左边的“设置”，下滚页面，找到“安全级别设置”，如下 安全级别设置 完全公开（推荐商家，客服等用户使用，代码中显示QQ号码，易于推广） 安全加密（推荐博主，论坛用户等使用，代码中不显示QQ号码） 选中“安全加密”，不然该选项默认的为完全公开，这样QQ号码就直接会显示在代码中，不利于隐私保护，选中之后，点击“保存”。保存之后，再次点击“推广工具”，即可发现下方的复制代码区域的HTML代码已看不到明码显示的QQ号，（若还是能看到QQ号，没有任何变化，可关闭该界面，重启浏览器重新进入该界面），本人的“复制这段代码并将其粘贴到您的网页上”下方区域的默认的代码为： 1&lt;a target="_blank" href="http://sighttp.qq.com/authd?IDKEY=b1afd83745b30922bc98e020847b86a5148d2114e62e8422"&gt;&lt;img border="0" src="http://wpa.qq.com/imgd?IDKEY=b1afd83745b30922bc98e020847b86a5148d2114e62e8422&amp;pic=52" alt="点击这里给我发消息" title="点击这里给我发消息"/&gt;&lt;/a&gt; 提取其中的href，即http://sighttp.qq.com/authd?IDKEY=b1afd83745b30922bc98e020847b86a5148d2114e62e8422，将该链接添加到主题配置文件中，具体如下： 12subnav: QQ: "http://sighttp.qq.com/authd?IDKEY=4faf682653b3b7f5f47b9cb6d2bb8b81de8fa7a8fb8cee12" 重新部署站点即可发现对应的QQ图标，点击该图标可直接给本人发临时QQ消息。 添加用户访问统计信息小工具——RevolverMaps 由于本人暂时不想搞SEO，所以就没有搞站点地图，更没有将本人的站点提交到百度和Google的站长平台上。但本人又想查看用户访问信息（是不是很矛盾o(╯□╰)o），而正好本人看到有个很酷炫的3D地球能满足本人的需求（其实很酷炫才是主要原因O(∩_∩)O~），所以本人决定将其加入本人的站点中（当做一部分装饰品๑乛◡乛๑）。该插件的名称为RevolverMaps，具体样式可以去其官网看，本人就不贴图了。设置完前三步之后，第四步让用户复制代码到自己的站点上，注意第四步会让你选“new map”还是“update”，由于本人是初次使用，当然是选择默认的“new map”，如果是以前使用过，就选择“update”，并将原来使用的script代码输入出现的文本框并提交，这样就只是更改3D地球样式而不会丢失用户访问信息数据。 具体添加方法为：将复制的script代码放入想显示的某个div中。本人得到的script代码为： &lt;script type=&quot;text/javascript&quot; src=&quot;//rf.revolvermaps.com/0/0/8.js?i=50om5cdoa3h&amp;amp;m=7&amp;amp;c=ff0000&amp;amp;cr1=ffffff&amp;amp;f=arial&amp;amp;l=49&quot; async=&quot;async&quot;&gt;&lt;/script&gt; 由于本人的博客是双栏的，本人当然是把RevolverMaps放入左栏中，本人刚开始是把得到的script代码放入主题文件夹下\layout_partial\left-col.ejs文件末尾的 12 &lt;/header&gt; &lt;/div&gt; &lt;/header&gt;标签之前（即在header的最下端显示RevolverMaps），但实际用起来有点不好看；本人又想干脆另外创造一个div放置地球，具体思路为：在birdhouse图标旁创建一个新的地球图标，再做一个像birdhouse图标一样的动画，鼠标移到地球图标时，出现一个div，该div用来放置RevolverMaps，这一步做到一半（即将一个新的地球图标并排放在birdhouse图标旁）发现这个效果感觉更不好看了，如果要改就需要大改了，有点麻烦o(︶︿︶)o唉；于是本人看到鼠标放在birdhouse图标出现的菜单栏上，想到何不如将该菜单栏在添加一栏，创建一个div用来显示RevolverMaps？事不宜迟，马上就动手添加该div，具体添加步骤如下： 首先当然是添加一个“访问情况”的列表名称，在主题文件夹下\layout\_partial\left-col.ejs文件中&lt;ul class=&quot;tips-inner&quot;&gt;下最后一个&lt;li&gt;后即&lt;/ul&gt;前添加&lt;li&gt;访问情况&lt;/li&gt;； 接着像其它的列表一样（点击该列表birdhouse图标就会改变成相应的图标），点击“访问情况”会将birdhouse图标改变成一个地球小图标，经查阅相应的css文件，其它的列表对应的图标好像是利用div的边框属性画出来的（某业余前端的猜测+_+），本人目前还没有这样的才能，就只有投机的采用Font Awesome中的globe图标了。在主题文件夹下\layout\_partial\left-col.ejs文件中&lt;div class=&quot;icon-ctn&quot;&gt;下最末尾即其对应的&lt;/div&gt;前添加： 123&lt;div class="icon-wrap icon-globe hide" data-idx="4"&gt; &lt;i class="fa fa-globe fa-spin fa-2x" aria-hidden="true"&gt;&lt;/i&gt;&lt;/div&gt; 这样点击“访问情况”会将birdhouse图标变成一个旋转的地球小图标了； 接下来就需要创建“访问情况”对应的div了，在主题文件夹下\layout\_partial\left-col.ejs文件中&lt;div class=&quot;switch-wrap&quot;&gt;下最末尾即其对应的&lt;/div&gt;前添加： 123&lt;section class="switch-part switch-part5"&gt; &lt;script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=50om5cdoa3h&amp;amp;m=7&amp;amp;c=ff0000&amp;amp;cr1=ffffff&amp;amp;f=arial&amp;amp;l=49" async="async"&gt;&lt;/script&gt;&lt;/section&gt; 这样点击“访问情况”就能出现酷炫的3D地球了，才怪:p。这样只能让3D地球出现在菜单界面，还需要添加修改相应的css； 最后就是改css样式了，本以为这一步很简单，没想到这一步花费本人最多时间╮(╯_╰)╭，修改的样式位于主题文件夹下\source\css_partial\main.styl文件中，首先为switch-part5添加对应的样式，在.switch-part4样式后添加： 123456.switch-part5&#123; left: 400%; width: 100%; //height: 200px; //margin-left: 47px; &#125; 做完这一步会发现3D地球显示不完全，下面会缺一点，所以还需要继续修改，修改过程如下：本人曾将该width减小（如上面代码中的注释），这样确实能让3D地球显示完全，但有点小，不是很好看；后面想到没显示完全可能是上层div（.switch-area）太小且设置了overflow: hidden;，于是这里本人首先增加了.switch-area的高度,这样确实能解决问题，但会使左栏的滚动条显示出来；所以本人接着尝试将.switch-area的overflow: hidden;注释掉，谁想注释掉之后出现了横向滚动条，这样更不好了，于是本人又更改为overflow-x: hidden;，谁想.switch-area又出现了竖直滚动条（感觉像拆东墙补西墙-_-|||），查阅相关资料（CSS-overflow特性及总结）得知若overflow-x为hidden，overflow-y不为hidden，则overflow-y将会自动重置为auto，所以这里不能这样改，但overflow: hidden;还是得注释掉，不然上层div撑不开，而且不增加高度的话，还是不能完全显示3D地球，因为超出就隐藏了嘛；因为注释掉之后会出现横向滚动条，而又不能修改.switch-area的overflow-x，所以就只能改更上层的div，这里本人突然想起上次给左栏添加滚动条时，在.left-col下添加了overflow: auto;，这次不如还修改这里，毕竟本人只想要竖直滚动条（其实不要滚动条却能滚动最好，但本人目前还没找到好的解决方案(╯﹏╰)b），不要横向滚动条，于是将其修改为： 12overflow-y: auto;overflow-x: hidden; 没想到这样也能解决问题，虽然还是会在左栏出现滚动条，但这样感觉比增加.switch-area的高度要好（嗯，应该要好吧(～ o ～)Y）。看以后能不能改成点击“访问情况”时才出现滚动条，点击其它列表则不出现滚动条（其实把滚动条隐藏最好，但网上那个两个div嵌套的方法本人尝试过会出现一些奇怪的问题，等以后再试试吧↖((&#94;ω^)↗）。 待续。。。 后记 目前就添加这些小组件，以后应该会陆续添加一些其它的小东西↖(&#94;ω&#94;)↗。 参考资料[1] 如何在自己网站上或者博客上放置QQ邮箱联系反馈（http://jingyan.baidu.com/tag?tagName=%E9%82%AE%E7%AE%B1 ） [2] 如何在自己的博客添加QQ组件（http://www.29mo.com/category/wltg ） [3] 一步一步教你给自己博客添加QQ在线（http://www.feizl.com/feizhuliu/QQbaodian/ ）]]></content>
      <categories>
        <category>建站小记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV中显著性检测算法的使用]]></title>
    <url>%2F2017%2F09%2F12%2FOpenCV%E4%B8%AD%E6%98%BE%E8%91%97%E6%80%A7%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文所用的OpenCV版本为opencv-3.2.0，编程语言为C++。 前言 OpenCV中实现了两种显著性检测算法，分别为Spectral Residual算法,出自Xiaodi Hou and Liqing Zhang. Saliency detection: A spectral residual approach. In Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on, pages 1–8. IEEE, 2007. 和 Fine Grained Saliency算法,出自Sebastian Montabone and Alvaro Soto. Human detection using a mobile platform and novel features derived from a visual saliency mechanism. In Image and Vision Computing, Vol. 28 Issue 3, pages 391–402. Elsevier, 2010.。这两种算法同样是在扩展包opencv_contrib-3.2.0中，也是由于opencv官方示例程序对初学者不友好（主要是本人境界不够o(╯□╰)o），所以本人对照其官方文档重新整理了一下。 说明篇 使用OpenCV中实现的显著性检测算法进行显著性检测十分方便简洁，利用以下三个函数就可以： 创建Spectral Residual算法显著性检测对象：static Ptr&lt;StaticSaliencySpectralResidual&gt; cv::saliency::StaticSaliencySpectralResidual::create(); Spectral Residual算法计算显著性图：bool cv::saliency::StaticSaliencySpectralResidual::computeSaliency(InputArray image, OutputArray saliencyMap); Fine Grained Saliency算法显著性检测对应的函数声明同Spectral Residual算法类似。 计算显著性图的二值图：bool cv::saliency::StaticSaliency::computeBinaryMap(InputArray _saliencyMap, OutputArray _binaryMap) ; 具体使用方法可参考实例篇。 实例篇 使用OpenCV中的显著性检测算法需要包含头文件#include &lt;opencv2/saliency.hpp&gt;，具体示例程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/saliency.hpp&gt;//******************************************************// [opencv_contrib/modules/saliency/src/saliency.cpp](https://github.com/opencv/opencv_contrib/blob/b7dcf141507edbe544e75820c76769a7769223ac/modules/saliency/src/saliency.cpp)////Ptr&lt;Saliency&gt; Saliency::create(const String&amp; saliencyType)//&#123;// if (saliencyType == "SPECTRAL_RESIDUAL")// return makePtr&lt;StaticSaliencySpectralResidual&gt;(); //computeSaliency返回的是32FC1// else if (saliencyType == "FINE_GRAINED")// return makePtr&lt;StaticSaliencyFineGrained&gt;(); //computeSaliency返回的是8UC1// else if (saliencyType == "BING")// return makePtr&lt;ObjectnessBING&gt;();// else if (saliencyType == "BinWangApr2014")// return makePtr&lt;MotionSaliencyBinWangApr2014&gt;();// return Ptr&lt;Saliency&gt;();//&#125;//// [opencv_contrib/modules/saliency/src/staticSaliency.cpp](https://github.com/opencv/opencv_contrib/blob/41b0a71ac826b1489d3e5c208ac7a95e58556caf/modules/saliency/src/staticSaliency.cpp)//computeBinaryMap()要求输入的saliencyMap为浮点数（eg:32FC1）//*****************************************************void spectralResidualTest()&#123; cv::Mat src_img = cv::imread("../data/true.png", CV_LOAD_IMAGE_ANYDEPTH | CV_LOAD_IMAGE_ANYCOLOR); // 载入最真实的原始图像 cv::namedWindow("src_img", CV_WND_PROP_ASPECTRATIO); cv::imshow("src_img", src_img); // [OpenCV实现显著性检测中的谱残差法（Spectral Residual Method）涉及到了傅立叶正反变换](http://blog.csdn.net/kena_m/article/details/49406687) if (src_img.empty()) exit(-1); if (src_img.channels() == 3) cv::cvtColor(src_img, src_img, CV_BGR2GRAY); cv::Mat planes[] = &#123; cv::Mat_&lt;float&gt;(src_img), cv::Mat::zeros(src_img.size(), CV_32F) &#125;; cv::Mat complex_img; //复数矩阵 merge(planes, 2, complex_img); //把单通道矩阵组合成复数形式的双通道矩阵 dft(complex_img, complex_img); // 使用离散傅立叶变换 //对复数矩阵进行处理，方法为谱残差 cv::Mat magnitude, phase_angle, mag_mean; cv::Mat real_part, imaginary_part; split(complex_img, planes); //分离复数到实部和虚部 real_part = planes[0]; //实部 imaginary_part = planes[1]; //虚部 cv::magnitude(real_part, imaginary_part, magnitude); //计算幅值 phase(real_part, imaginary_part, phase_angle); //计算相角 float *pre, *pim, *pm, *pp; //对幅值进行对数化 for (int i = 0; i &lt; magnitude.rows; i++) &#123; pm = magnitude.ptr&lt;float&gt;(i); for (int j = 0; j &lt; magnitude.cols; j++) &#123; *pm = log(*pm); pm++; &#125; &#125; blur(magnitude, mag_mean, cv::Size(5, 5)); //对数谱的均值滤波 magnitude = magnitude - mag_mean; //求取对数频谱残差 //把对数谱残差的幅值和相角划归到复数形式 for (int i = 0; i &lt; magnitude.rows; i++) &#123; pre = real_part.ptr&lt;float&gt;(i); pim = imaginary_part.ptr&lt;float&gt;(i); pm = magnitude.ptr&lt;float&gt;(i); pp = phase_angle.ptr&lt;float&gt;(i); for (int j = 0; j &lt; magnitude.cols; j++) &#123; *pm = exp(*pm); *pre = *pm * cos(*pp); *pim = *pm * sin(*pp); pre++; pim++; pm++; pp++; &#125; &#125; cv::Mat planes1[] = &#123; cv::Mat_&lt;float&gt;(real_part), cv::Mat_&lt;float&gt;(imaginary_part) &#125;; merge(planes1, 2, complex_img); //重新整合实部和虚部组成双通道形式的复数矩阵 idft(complex_img, complex_img, cv::DFT_SCALE); // 傅立叶反变换 split(complex_img, planes); //分离复数到实部和虚部 real_part = planes[0]; imaginary_part = planes[1]; cv::magnitude(real_part, imaginary_part, magnitude); //计算幅值和相角 for (int i = 0; i &lt; magnitude.rows; i++) &#123; pm = magnitude.ptr&lt;float&gt;(i); for (int j = 0; j &lt; magnitude.cols; j++) &#123; *pm = (*pm) * (*pm); pm++; &#125; &#125; GaussianBlur(magnitude, magnitude, cv::Size(7, 7), 2.5, 2.5); cv::Mat invDFT, invDFTcvt; normalize(magnitude, invDFT, 0, 255, cv::NORM_MINMAX); //归一化到[0,255]供显示 invDFT.convertTo(invDFTcvt, CV_8U); //转化成CV_8U型 cv::namedWindow("SpectualResidual", CV_WND_PROP_ASPECTRATIO); cv::imshow("SpectualResidual", invDFTcvt); cv::Mat thresholded; cv::threshold(invDFTcvt, thresholded, 0, 255, CV_THRESH_OTSU); cv::namedWindow("Thresholded Image", CV_WND_PROP_ASPECTRATIO); cv::imshow("Thresholded Image", thresholded); cv::Mat eroded; // 纵向腐蚀 cv::erode(thresholded, eroded, cv::Mat(5, 1, CV_8UC1, cv::Scalar(1)), cv::Point(-1, -1), 3); // cv::Point(-1,-1)为默认参数，代表原点（描点）为矩阵中心 cv::namedWindow("eroded Image", CV_WND_PROP_ASPECTRATIO); cv::imshow("eroded Image", eroded); //cv::Mat thresholded; cv::threshold(eroded, thresholded, 60, 255, CV_THRESH_BINARY); cv::namedWindow("Thresholded eroded Image", CV_WND_PROP_ASPECTRATIO); cv::imshow("Thresholded eroded Image", thresholded);&#125; // 显著性检测算法基类void saliencyTest()&#123; cv::Mat src_img = cv::imread("../data/true.png", CV_LOAD_IMAGE_ANYDEPTH | CV_LOAD_IMAGE_ANYCOLOR); cv::namedWindow("src_img", CV_WND_PROP_ASPECTRATIO); cv::imshow("src_img", src_img); if (src_img.empty()) exit(-1); if (src_img.channels() == 3) cv::cvtColor(src_img, src_img, CV_BGR2GRAY); cv::Ptr&lt;cv::saliency::Saliency&gt; saliency_algorithm = cv::saliency::Saliency::create("SPECTRAL_RESIDUAL"); // FINE_GRAINED为Fine Grained Saliency算法 cv::Mat saliency_map; if (saliency_algorithm-&gt;computeSaliency(src_img, saliency_map)) // 计算显著性图 &#123; cv::namedWindow("SR saliency map", CV_WND_PROP_ASPECTRATIO); cv::imshow("SR saliency map", saliency_map); cv::Mat saliency_map_show(saliency_map.size(), CV_8UC1); normalize(saliency_map, saliency_map_show, 0, 255, CV_MINMAX); //归一化到[0,255]供显示 saliency_map_show.convertTo(saliency_map_show, CV_8U); //转化成CV_8U型 cv::namedWindow("saliency_map_show", CV_WND_PROP_ASPECTRATIO); cv::imshow("saliency_map_show", saliency_map_show); cv::Mat binary_map; cv::saliency::StaticSaliencySpectralResidual spec; if (spec.computeBinaryMap(saliency_map, binary_map)) // 对显著性图进行二值化 &#123; cv::namedWindow("binary map", CV_WND_PROP_ASPECTRATIO); cv::imshow("binary map", binary_map); &#125; &#125;&#125;// Fine Grained Saliency算法void FGSTest()&#123; cv::Mat src_img = cv::imread("../data/true.png", CV_LOAD_IMAGE_ANYDEPTH | CV_LOAD_IMAGE_ANYCOLOR); cv::namedWindow("src_img", CV_WND_PROP_ASPECTRATIO); cv::imshow("src_img", src_img); if (src_img.empty()) exit(-1); if (src_img.channels() == 3) cv::cvtColor(src_img, src_img, CV_BGR2GRAY); cv::Ptr&lt;cv::saliency::StaticSaliencyFineGrained&gt; fgs = cv::saliency::StaticSaliencyFineGrained::create(); cv::Mat fgs_saliency_map; fgs-&gt;computeSaliency(src_img, fgs_saliency_map); cv::namedWindow("FGS saliency map", CV_WND_PROP_ASPECTRATIO); cv::imshow("FGS saliency map", fgs_saliency_map); //cv::imwrite("../data/T_S.png", fgs_saliency_map); cv::Mat binary_map; cv::threshold(fgs_saliency_map, binary_map, 0, 255, CV_THRESH_OTSU); cv::namedWindow("binary map", CV_WND_PROP_ASPECTRATIO); cv::imshow("binary map", binary_map); //cv::imwrite("../data/T_S_B.png", binary_map);&#125;// Spectral Residual算法void SRTest()&#123; cv::Mat src_img = cv::imread("../data/true.png", CV_LOAD_IMAGE_ANYDEPTH | CV_LOAD_IMAGE_ANYCOLOR); cv::namedWindow("src_img", CV_WND_PROP_ASPECTRATIO); cv::imshow("src_img", src_img); if (src_img.empty()) exit(-1); if (src_img.channels() == 3) cv::cvtColor(src_img, src_img, CV_BGR2GRAY); cv::Ptr&lt;cv::saliency::StaticSaliencySpectralResidual&gt; sr = cv::saliency::StaticSaliencySpectralResidual::create(); cv::Mat sr_saliency_map; sr-&gt;computeSaliency(src_img, sr_saliency_map); cv::namedWindow("SR saliency map", CV_WND_PROP_ASPECTRATIO); cv::imshow("SR saliency map", sr_saliency_map); cv::Mat binary_map; sr-&gt;computeBinaryMap(sr_saliency_map, binary_map); cv::namedWindow("binary map", CV_WND_PROP_ASPECTRATIO); cv::imshow("binary map", binary_map);&#125;int main(int argc, char *argv[])&#123; //spectralResidualTest(); //saliencyTest(); //FGSTest(); SRTest(); while (cv::waitKey(0) != 27) &#123; &#125; return 0;&#125; 这里面有个小东西需要注意，就是computeBinaryMap()函数，看其文档描述其中使用K-means算法和Otsu算法对显著性图进行二值化处理，其输入的显著性图数据类型应该为浮点数，OpenCV中Spectral Residual算法computeSaliency()返回的结果为浮点数，而Fine Grained Saliency算法computeSaliency()返回的结果却是整型数据，所以这一点需要注意Fine Grained Saliency算法返回的结果不能直接使用computeBinaryMap()函数，一般对其结果直接使用OTSU算法进行阈值分割即可。 后记 本文使用的这两种算法在本人的电脑上运行时间都较长，基本不可能用来处理视频流，而且在本人的这次实验中效果也不太理想，毕竟这是用来处理静态图像的两种显著性方法。不过OpenCV中也有用来处理视频流的显著性检测算法，其为BING算法,出自Ming-Ming Cheng, Ziming Zhang, Wen-Yan Lin, and Philip Torr. Bing: Binarized normed gradients for objectness estimation at 300fps. In IEEE CVPR, 2014.，实际上这是一种快速提取目标候选框的算法。]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenCV中Selective Search算法的使用]]></title>
    <url>%2F2017%2F09%2F10%2FOpenCV%E4%B8%ADSelective-Search%E7%AE%97%E6%B3%95%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文所用的OpenCV版本为opencv-3.2.0，编程语言为C++。 前言 OpenCV-3.2中的Selective Search算法是在其扩展包中，所以要想使用该算法需自行编译opencv_contrib-3.2.0。由于扩展包中的示例程序有点简陋，对初学者也不友好（本人编程水平有限，粗浅评价，勿怪(*&#94;__&#94; *) 嘻嘻……），所以本人参考其官方文档及其官方示例程序写下此文。 说明篇 该算法是选取region proposal（一般翻译成候选区域 / 区域建议）领域中的state-of-the-art。其算法具体思想出自Jasper RR Uijlings, Koen EA van de Sande, Theo Gevers, and Arnold WM Smeulders. Selective search for object recognition. International journal of computer vision, 104(2):154–171, 2013.，若英文水平不够，还想了解其中文思想请参考文末参考资料。 OpenCV中实现的相应函数： void cv::ximgproc::segmentation::SelectiveSearchSegmentation::addGraphSegmentation(Ptr&lt;GraphSegmentation&gt; g);：添加相应的图割算法； void cv::ximgproc::segmentation::SelectiveSearchSegmentation::addImage(InputArray img) ; ：添加待处理的图片； void cv::ximgproc::segmentation::SelectiveSearchSegmentation::addStrategy(Ptr&lt;SelectiveSearchSegmentationStrategy&gt; s); ：添加相应的策略（颜色相似度、纹理相似度、尺寸相似度和填充相似度）； void cv::ximgproc::segmentation::SelectiveSearchSegmentation::process(std::vector&lt;Rect&gt; &amp;rects);：结合图割算法和相应策略进行处理，返回候选框。 实例篇 使用Selective Search算法需包含#include &lt;opencv2/ximgproc.hpp&gt;，完整示例程序如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/ximgproc.hpp&gt;void SSTest()&#123; // [Image segmentation](http://docs.opencv.org/3.2.0/d5/df0/group__ximgproc__segmentation.html#ga5e3e721c5f16e34d3ad52b9eeb6d2860) cv::Mat src_img = cv::imread("../data/true.png", CV_LOAD_IMAGE_ANYDEPTH | CV_LOAD_IMAGE_ANYCOLOR); // 载入原始图像 cv::namedWindow("src_img", CV_WINDOW_KEEPRATIO); cv::imshow("src_img", src_img); //// 转换为灰度图 //cv::Mat gray_img; //cvtColor(src_img, gray_img, cv::COLOR_BGR2GRAY); // 图割算法 cv::Ptr&lt;cv::ximgproc::segmentation::GraphSegmentation&gt; gs = cv::ximgproc::segmentation::createGraphSegmentation(); cv::Mat graph_segmented; gs-&gt;processImage(src_img, graph_segmented); normalize(graph_segmented, graph_segmented, 0, 255, CV_MINMAX); // 归一化到[0,255]供显示 graph_segmented.convertTo(graph_segmented, CV_8U); // 数据类型转化成CV_8U型 // cvtColor(graph_segmented, graph_segmented, CV_GRAY2BGR); cv::namedWindow("graph_segmented", CV_WINDOW_KEEPRATIO); imshow("graph_segmented", graph_segmented); // 为selective search算法添加图割算法处理结果 cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentation&gt; ss = cv::ximgproc::segmentation::createSelectiveSearchSegmentation(); ss-&gt;addGraphSegmentation(gs); ss-&gt;addImage(src_img); // 添加待处理的图片 // 自定义策略 cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy&gt; sss_color = cv::ximgproc::segmentation::createSelectiveSearchSegmentationStrategyColor(); // 颜色相似度策略 cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy&gt; sss_texture = cv::ximgproc::segmentation::createSelectiveSearchSegmentationStrategyTexture(); // 纹理相似度策略 cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy&gt; sss_size = cv::ximgproc::segmentation::createSelectiveSearchSegmentationStrategySize(); // 尺寸相似度策略 cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy&gt; sss_fill = cv::ximgproc::segmentation::createSelectiveSearchSegmentationStrategyFill(); // 填充相似度策略 // 添加策略 cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentationStrategy&gt; sss = cv::ximgproc::segmentation::createSelectiveSearchSegmentationStrategyMultiple(sss_color, sss_texture, sss_size, sss_fill); // 合并以上4种策略 ss-&gt;addStrategy(sss); std::vector&lt;cv::Rect&gt; regions; ss-&gt;process(regions); // 处理结果 // 显示结果 cv::Mat show_img = src_img.clone(); for (std::vector&lt;cv::Rect&gt;::iterator it_r = regions.begin(); it_r != regions.end(); ++it_r) &#123; cv::rectangle(show_img, *it_r, cv::Scalar(0, 0, 255), 3); &#125; cv::namedWindow("show_img", CV_WINDOW_KEEPRATIO); imshow("show_img", show_img); // -------忽略上述步骤，直接采用方便算法提取候选区域------------------------ /*************************************************************************** cv::Ptr&lt;cv::ximgproc::segmentation::SelectiveSearchSegmentation&gt; ss = cv::ximgproc::segmentation::createSelectiveSearchSegmentation(); ss-&gt;setBaseImage(src_img); // 采用switch* functions提取候选区域 ss-&gt;switchToSelectiveSearchFast(); // 快速提取区域 std::vector&lt;cv::Rect&gt; rects; ss-&gt;process(rects); int nb_rects = 10; char c = (char)cv::waitKey(); while (c != 'q') &#123; cv::Mat wimg = src_img.clone(); int i = 0; for (std::vector&lt;cv::Rect&gt;::iterator it = rects.begin(); it != rects.end(); ++it) &#123; if (i++ &lt; nb_rects) &#123; cv::rectangle(wimg, *it, cv::Scalar(0, 0, 255), 3); &#125; &#125; cv::namedWindow("Output", CV_WINDOW_KEEPRATIO); imshow("Output", wimg); c = (char)cv::waitKey(); if (c == 'd') &#123; nb_rects += 10; &#125; if (c == 'a' &amp;&amp; nb_rects &gt; 10) &#123; nb_rects -= 10; &#125; &#125; ********************************************************/&#125;int main(int argc, char *argv[])&#123; SSTest(); while (cv::waitKey(0) != 27) &#123;&#125; return 0;&#125; 后记 使用该算法，要想达到理想效果，一般需要调整图割算法的参数或注释中方法switchToSelectiveSearchFast()的参数。本人的这次实验为了达到理想的选取的效果，其调整参数花了不少时间，而且该算法运行时间在本人电脑上略显长。GitHub上也有大神自己用opencv实现了该算法，参考watanika/selective-search-cpp，该算法的参数感觉比OpenCV自带的Selective Search算法要好调一些，但优化效果没有opencv好，其运行时间在本人电脑上更长。 参考资料[1] 论文笔记：Selective Search for Object Recognition（http://jermmy.xyz/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/ ） [2] Selective Search for Object Recognition(阅读)（http://blog.csdn.net/langb2014/article/category/5772811 ） [3] 论文笔记 《Selective Search for Object Recognition》（http://blog.csdn.net/csyhhb/article/category/6048588 ）]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10以树形结构显示文件目录结构]]></title>
    <url>%2F2017%2F09%2F05%2FWin10%E4%BB%A5%E6%A0%91%E5%BD%A2%E7%BB%93%E6%9E%84%E6%98%BE%E7%A4%BA%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[前言 本文其实可以算是标题党，Windows本身并不能以树形结构显示文件目录结构，一般需要借助第三方工具（后面去网上搜索了一下，发现Windows居然也有一个tree命令o(╯□╰)o），Windows虽然能用命令行显示树形结构文件目录，但不像Linux那样可以输入一些参数控制其输出。Win10有个特殊的功能，就是可以使用Ubuntu的bash，只需要开启这个有趣的功能，就可以将Win10当Ubuntu使用，从而像Linux那样只输入相关命令即可显示树形结构文件目录。 *注：值得注意的是Win10中的bash目前不支持中文输入，只能切换到英文输入才能正常输入。 准备篇首先需要在Win10下开启bash功能。具体开启方法为： 打开 Win图标 ==》 设置 ==》 更新和安全 ==》 针对开发人员（左侧），选中开发人员模式， 打开 Win图标 ==》 设置 ==》 应用 ==》 应用和功能（左侧） ==》 程序和功能（最下面的相关设置中） ==》 启用或关闭Windows功能（左侧），选中适用于Linux的Windows子系统(Beta)后点击确定。 重启计算机。打开bash，打开bash的方法很多，这里列出三种：1、直接在微软小娜中输入关键字”bash“搜索Bash on Ubuntu on Windows；2、Win键+R，输入bash，点击确定即可打开bash；3、Win键+R，输入cmd，在cmd中输入bash，回车即可打开bash。打开bash后将会提示你是否下载安装Ubuntu on Windows，输入y继续，稍等片刻即可完成下载安装。 设置篇 安装完成后系统将会提示你设置用户名和密码。（如果这一步设置成功可以直接跳过设置篇直接看使用篇）。不知道怎的，本人这一步没有完成，每次系统都是直接以root用户登录，而且没有密码，为了安全考虑，也幸好登录时是root用户，可以自由对系统修改。所以本人需要对root密码进行修改，并创建新的用户。具体过程需执行以下命令： root用户下，修改用户密码： 1passwd 用户名 (修改密码) 由于本人需要修改root密码，所以该用户名即为root，执行之后需要输入新密码（在*nix哲学中，密码是不会显示在输入屏幕中的，所以如果在输入密码时发现屏幕没有任何变化是没关系的，只管输入即可↖(&#94;ω&#94;)↗），两次输入完成后会显示密码更新成功。 接下来需要创建新的普通用户，在root用户下执行： 1adduser xxx # 这样的命令会在home目录下添加一个帐号 或者 1useradd xxx #仅仅是添加用户，不会在home目录添加帐号 推荐使用前者，这样可以很明确已经成功创建新用户，而且如果用户需要存放一些文件也更安全和方便。 在*nix中，绝对不推荐直接使用root用户对系统执行各种命令，毕竟其权限太大，一旦误操作将造成无法挽回的后果。有些命令普通用户可能没有权限执行，这时需要提高其权限，普通用户临时获取root权限的方法为：在需要执行的命令前添加sudo，像上文中如果普通用户需要创建新用户xxx则需要执行sudo adduser xxx，执行以上命令后同样需要输入新用户的密码。 使用篇 先切换至普通用户，执行su xxx切换用户，即可发现shell提示符由#变为$，前面的用户名由root变为xxx；执行cd ~切换至用户目录。由于Ubuntu系统中本身没有tree这个命令，需要执行以下命令安装tree命令工具： 1sudo apt install tree 直接输入tree命令，系统将会自动以树形结构列出当前目录中所有文件及文件夹；执行tree -L N 命令，以树形结构查看当前N级的目录和文件，eg：以树形结构查看当前目录二级文件结构，则执行tree -L 2。若想将输出的2级文件结构保存至上一层文件的tree.txt文件中，可执行tree -L 2 &gt; ../tree.txt，进入上一层目录cd ..，打开tree.txt即可发现该目录的文件结构。 后记 遇事还是需要多查证一下啊，想当然果然是会出问题的，文章开头差点就犯错误了~(≧▽≦)/~。本文其实是在写Win10＋VS2013＋CMake-gui编译和配置OpenCV-3.2.0时，为了方便显示输出文件结构而查找的相关资料。 参考资料[1]linux tree命令以树形结构显示文件目录结构（http://jingyan.baidu.com/tag?tagName=linux ） [2] win tree命令 tree导出目录 tree显示树形结构（http://jingyan.baidu.com/tag?tagName=%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F ） [3] win10下linux系统的安装（开启）和使用 [4] Ubuntu建立和删除用户 [5] linux修改root密码和linux忘记root密码后找回密码的方法]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>unix-like</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo添加站内本地搜索]]></title>
    <url>%2F2017%2F09%2F01%2FHexo%E6%B7%BB%E5%8A%A0%E7%AB%99%E5%86%85%E6%9C%AC%E5%9C%B0%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[前言 虽然本人博客目前数量不多，质量也不高，但抱着搞事的心态，先弄它一个站内本地搜索再说。 准备篇 要想使用本地搜索功能，首先需要安装相应的搜索插件hexo-generator-searchdb，网上可能大多数用的是hexo-generator-search这个插件，也有都装的，但本人就只安装这一个了，好像hexo-generator-searchdb更完善一点，由于本人前端接触的极少，所以就没有一一对比了，网上也没查到具体对比情况，有兴趣的童靴可以试试(╯▽╰)。至于具体安装如下，在站点根目录执行： 1npm install hexo-generator-searchdb --save 安装完之后重新生成页面，将会发现public文件夹下多出一个search.xml文件。然后在配置文件_config.yml中添加： 1234# 站点本地搜索search: path: search.xml field: all 其中： path - 指定生成的索引数据的文件名。默认为 search.xml 。 field - 指定索引数据的生成范围。可选值包括： post - 只生成博客文章（post）的索引（默认）； page - 只生成其他页面（page）的索引； all - 生成所有文章和页面的索引。 至于是在主题配置文件，还是在站点配置文件中添加，个人觉得都没关系，附：本人是在主题配置文件中添加的。 接下来就需要修改原主题的代码了。 改码篇 由于本人博客主题是基于SPFK对照着black-blue进行修改的，而且因为black-blue是有搜索的（本人不知道black-blue主题的作者是如何完成的，借助了什么技术），所以本人就看black-blue的搜索功能是修改了SPFK哪个地方，再将相应的代码添加至SPFK中（其中相应的代码来自让 Hexo 博客支持本地站内搜索），从而逐渐完成本次搜索功能。 首先找到spfk主题下的left-col.ejs文件，对其修改如下： 123456789101112&lt;% if (theme.search_box)&#123; %&gt; &lt;!-- &lt;form&gt; &lt;input type=&quot;text&quot; class=&quot;st-default-search-input search&quot; id=&quot;search&quot; placeholder=&quot; Search...&quot;&gt; &lt;/form&gt; --&gt; &lt;form id=&quot;search-form&quot;&gt; &lt;!-- 搜索框相关 --&gt; &lt;input type=&quot;text&quot; id=&quot;local-search-input&quot; name=&quot;q&quot; results=&quot;0&quot; placeholder=&quot;Search...&quot; class=&quot;search form-control&quot; autocomplete=&quot;off&quot; autocorrect=&quot;off&quot;/&gt; &lt;i class=&quot;fa fa-times&quot; onclick=&quot;resetSearch()&quot;&gt;&lt;/i&gt; &lt;!-- 清空/重置搜索框 --&gt; &lt;/form&gt; &lt;div id=&quot;local-search-result&quot;&gt;&lt;/div&gt; &lt;!-- 搜索结果区 --&gt; &lt;p class=&apos;no-result&apos;&gt;No results found &lt;/p&gt; &lt;!-- 无匹配时显示，注意请在 CSS 中设置默认隐藏 --&gt; &lt;%&#125;%&gt; 其次找到spfk主题下的after-footer.ejs文件，将其修改如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140&lt;% if (theme.search_box)&#123; %&gt; &lt;!-- &lt;script type=&quot;text/javascript&quot;&gt; window.onload = function()&#123; document.getElementById(&quot;search&quot;).onclick = function()&#123; console.log(&quot;search&quot;) search(); &#125; &#125; function search()&#123; (function(w,d,t,u,n,s,e)&#123;w[&apos;SwiftypeObject&apos;]=n;w[n]=w[n]||function()&#123; (w[n].q=w[n].q||[]).push(arguments);&#125;;s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); &#125;)(window,document,&apos;script&apos;,&apos;//s.swiftypecdn.com/install/v2/st.js&apos;,&apos;_st&apos;); _st(&apos;install&apos;,&apos;A1Pz-LKMXbrzcFg2FWi6&apos;,&apos;2.0.0&apos;); &#125; &lt;/script&gt; --&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 激活搜索框时才搜索 var inputArea = document.querySelector(&quot;#local-search-input&quot;); var getSearchFile = function()&#123; // 调用搜索函数 var search_path = &quot;&lt;%- config.search.path %&gt;&quot;; if (search_path.length == 0) &#123; search_path = &quot;search.xml&quot;; &#125; var path = &quot;&lt;%- config.root %&gt;&quot; + search_path; searchFunc(path, &apos;local-search-input&apos;, &apos;local-search-result&apos;); &#125; inputArea.onfocus = function()&#123; getSearchFile() &#125; // 搜索重置 var $resetButton = $(&quot;#search-form .fa-times&quot;); var $resultArea = $(&quot;#local-search-result&quot;); inputArea.oninput = function()&#123; $resetButton.show(); &#125; resetSearch = function()&#123; $resultArea.html(&quot;&quot;); document.querySelector(&quot;#search-form&quot;).reset(); $resetButton.hide(); $(&quot;.no-result&quot;).hide(); &#125; // 屏蔽回车 inputArea.onkeydown = function()&#123; if(event.keyCode==13) return false&#125; // 无搜索结果 $resultArea.bind(&quot;DOMNodeRemoved DOMNodeInserted&quot;, function(e) &#123; if (!$(e.target).text()) &#123; $(&quot;.no-result&quot;).show(200); &#125; else &#123; $(&quot;.no-result&quot;).hide(); &#125; &#125;) // 搜索函数 var searchFunc = function(path, search_id, content_id) &#123; &apos;use strict&apos;; $.ajax(&#123; url: path, dataType: &quot;xml&quot;, success: function( xmlResponse ) &#123; // get the contents from search data var datas = $( &quot;entry&quot;, xmlResponse ).map(function() &#123; return &#123; title: $( &quot;title&quot;, this ).text(), content: $(&quot;content&quot;,this).text(), url: $( &quot;url&quot; , this).text() &#125;; &#125;).get(); var $input = document.getElementById(search_id); var $resultContent = document.getElementById(content_id); $input.addEventListener(&apos;input&apos;, function()&#123; var str=&apos;&lt;ul class=\&quot;search-result-list\&quot;&gt;&apos;; var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/); $resultContent.innerHTML = &quot;&quot;; if (this.value.trim().length &lt;= 0) &#123; return; &#125; // perform local searching datas.forEach(function(data) &#123; var isMatch = true; var content_index = []; var data_title = data.title.trim().toLowerCase(); var data_content = data.content.trim().replace(/&lt;[^&gt;]+&gt;/g,&quot;&quot;).toLowerCase(); var data_url = data.url; var index_title = -1; var index_content = -1; var first_occur = -1; // only match artiles with not empty titles and contents if(data_title != &apos;&apos; &amp;&amp; data_content != &apos;&apos;) &#123; keywords.forEach(function(keyword, i) &#123; index_title = data_title.indexOf(keyword); index_content = data_content.indexOf(keyword); if( index_title &lt; 0 &amp;&amp; index_content &lt; 0 )&#123; isMatch = false; &#125; else &#123; if (index_content &lt; 0) &#123; index_content = 0; &#125; if (i == 0) &#123; first_occur = index_content; &#125; &#125; &#125;); &#125; // show search results if (isMatch) &#123; str += &quot;&lt;li&gt;&lt;a href=&apos;/&quot;+ data_url +&quot;&apos; class=&apos;search-result-title&apos; target=&apos;_blank&apos;&gt;&quot;+ &quot;&gt; &quot; + data_title +&quot;&lt;/a&gt;&quot;; var content = data.content.trim().replace(/&lt;[^&gt;]+&gt;/g,&quot;&quot;); if (first_occur &gt;= 0) &#123; // cut out characters var start = first_occur - 6; var end = first_occur + 6; if(start &lt; 0)&#123; start = 0; &#125; if(start == 0)&#123; end = 10; &#125; if(end &gt; content.length)&#123; end = content.length; &#125; var match_content = content.substr(start, end); // highlight all keywords keywords.forEach(function(keyword)&#123; var regS = new RegExp(keyword, &quot;gi&quot;); match_content = match_content.replace(regS, &quot;&lt;em class=\&quot;search-keyword\&quot;&gt;&quot;+keyword+&quot;&lt;/em&gt;&quot;); &#125;) str += &quot;&lt;p class=\&quot;search-result\&quot;&gt;&quot; + match_content +&quot;...&lt;/p&gt;&quot; &#125; &#125; &#125;) $resultContent.innerHTML = str; &#125;) &#125; &#125;) &#125; &lt;/script&gt;&lt;%&#125;%&gt; 最后找到spfk主题下的main.styl文件，在其末尾添加： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798/*搜索框*/.search &#123; width: 68%; height: 18px; margin-top: 1px; padding: 0; font-family: inherit; border: 2px solid transparent; border-bottom: 2px solid #d3d3d3; border-radius: 2px; opacity: 0.65; background: none;&#125;.search:hover &#123; border: 2px solid #d3d3d3; opacity: 1; box-shadow: 0 0 10px rgba(0,0,0,0.3);&#125;/*搜索重置按钮*/#search-form .fa-times &#123; display: none; padding: 1px 0.7em; box-shadow: 0 0 3px rgba(0,0,0,0.15); cursor: pointer; color: #4094c7;&#125;#search-form .fa-times:active &#123; background: #d3d3d3;&#125;#search-form .fa-times:hover &#123; zoom: 1.1; padding: 1px 0.6em; border: 1px solid #d3d3d3; box-shadow: 0 0 6px rgba(0,0,0,0.25);&#125;/*搜索结果区*/#local-search-result &#123; //margin: auto -12% auto -6%; margin: 0; font-size: 0.9em; text-align: left; word-break: break-all; box-shadow: 4px 4px 6px rgba(0,0,0,0.46);&#125;#local-search-result ul.search-result-list li:hover &#123; font-weight: normal;&#125;/*单条搜索结果*/#local-search-result li &#123; margin: 0.5em auto; border-bottom: 2px solid #d3d3d3;&#125;#local-search-result .search-result-list li:hover &#123; background: rgba(47,46,46,0.8); box-shadow: 0 0 5px rgba(0,0,0,0.2);&#125;/*匹配的标题*/#local-search-result a.search-result-title &#123; line-height: 1.2; font-weight: bold; color: #4094c7;&#125;/*搜索预览段落*/#local-search-result p.search-result &#123; margin: 0.4em auto; line-height: 1.2em; max-height: 3.6em; overflow: hidden; font-size: 0.8em; text-align: justify; color: #ffffffb3;&#125;/*匹配的关键词*/#local-search-result em.search-keyword &#123; color: #f58e90; border-bottom: 1px dashed #f58e90; font-weight: bold; font-size: 1em;&#125;/*无匹配搜索结果时显示*/p.no-result &#123; display: none; margin: 2em 0 2em 6%; padding-bottom: 0.5em; text-align: left; color: #808080; font-family: font-serif serif; border-bottom: 2px solid #d3d3d3;&#125; 这里请注意，当对main.styl文件做以上修改时，可能会发现有两个.search样式，而且相差不大，这时，不要对其原有的.search进行修改，更不要去注释掉它，只做上述修改就不用管了，不然可能会发生一些奇怪的事o(&gt;﹏&lt;)o。本人当时做以上修改时，将其原有的.search样式注释掉之后，整个页面的css布局全部都乱了(╯﹏╰)，不知道为什么(⊙_⊙?)，这两个同名样式看起来明明差不多的，最后只能维持现状了，等以后有机会再看看吧，业余前端伤不起啊!╮(╯_╰)╭。 至此整个站内本地搜索功能基本完成，勉强可以使用站内搜索功能了。 问题篇 *注：以下问题目前都没解决╮(╯▽╰)╭。 1、搜索函数返回的url地址有问题。 问题描述：当点击搜索结果时，新弹出的标签页地址栏中url地址会有部分乱码情况；当鼠标移到搜索的结果列表上时，浏览器左下角显示的url地址虽然没有乱码情况，但其中有一个重复的/符号。所幸这两个问题并没有造成浏览器解析错误，浏览器还是可以正常显示页面的。 2、搜索结果区布局有问题。 问题描述：当显示搜索结果时，搜索结果区会上下扩张，从而将其上下本来存在的一些布局挤开，造成布局混乱。这其实不算是一个spfk主题或者新添加的搜索功能的问题，而是新添加的一个东西又没有相应的和原本布局结合的布局文件，那就极大可能会有布局混乱的问题，至于这个要和原本布局契合的搜索结果区布局文件就只有等本人以后有机会有时间再完善去喽╮(╯▽╰)╭。 3、搜索框激活问题。 问题描述：搜索框激活延迟很大，有时过很久或者需要切换站内页面它才能激活，给人的感觉就是好像没有搜索功能似的。添加搜索框激活功能据作者MOxFIVE所说是为了不让索引文件影响页面加载速度，MOxFIVE同时也在文末指出了一些不足之处，如果索引文件太大，可能还是会造成一些问题，但本人的博客数量又不多，所以估计还是本人的代码混合问题，而且MOxFIVE的博客搜索功能好像没这个问题（至少本人目前没发现）。这个问题同样只有等以后再说了(*&#94;__&#94;*) 嘻嘻……。 后记 本文添加的本地搜索还很粗糙，还有很多地方需要以后去完善。但这好歹是一个好的开始，搜索功能至少勉强能够正常使用，总比以前是个空壳要好，以后有机会再慢慢去去完善吧↖(&#94;ω&#94;)↗。 参考资料[1] jQuery-based Local Search Engine for Hexo（http://www.hahack.com/categories/codes/ ） [2] 让 Hexo 博客支持本地站内搜索（http://moxfive.xyz/tags/Hexo/ ） [3] Hexo博客添加站内搜索（https://www.ezlippi.com/categories/hexo/ ） [4] Hexo本地搜索及部分SEO优化 （https://www.oyohyee.com/categories/Note/ ）]]></content>
      <categories>
        <category>建站小记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用OpenCV显示OpenGL图形]]></title>
    <url>%2F2017%2F08%2F31%2F%E7%94%A8OpenCV%E6%98%BE%E7%A4%BAOpenGL%E5%9B%BE%E5%BD%A2%2F</url>
    <content type="text"><![CDATA[前言 本文就是一个小实验，试验OpenCV到底能不能支持OpenGL图形显示。 正文 如果在OpenCV用CMake编译时勾选WITH_OPENGL且编译一切顺利的话，编译和配置的具体步骤和情况可以看本人写的一篇文档：Win10＋VS2013＋CMake-gui编译和配置OpenCV-3.2.0 ，那么就可以用OpenCV窗口显示OpenGL图形。 在VS下使用Windows原有的OpenGL函数需要包含以下头文件和库文件： 123456#include &lt;Windows.h&gt;#include &lt;GL/gl.h&gt;#include &lt;GL/glu.h&gt;#pragma comment(lib, "OpenGL32.lib")#pragma comment(lib, "glu32.lib") 在OpenCV中显示OpenGL图形需要cv::namedWindow(openGLWindowName, cv::WINDOW_OPENGL)，在namedWindow函数中添加cv::WINDOW_OPENGL参数说明该窗口支持OpenGL图形。 附示例程序： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299#include &lt;opencv.hpp&gt;#include &lt;Windows.h&gt;#include &lt;GL/gl.h&gt;#include &lt;GL/glu.h&gt;#pragma comment(lib, "OpenGL32.lib")#pragma comment(lib, "glu32.lib")static const float vertex_list[][3] =&#123; -0.5f, -0.5f, -0.5f, 0.5f, -0.5f, -0.5f, -0.5f, 0.5f, -0.5f, 0.5f, 0.5f, -0.5f, -0.5f, -0.5f, 0.5f, 0.5f, -0.5f, 0.5f, -0.5f, 0.5f, 0.5f, 0.5f, 0.5f, 0.5f,&#125;;// 将要使用的顶点的序号保存到一个数组里面 static const GLint index_list[][2] =&#123; &#123; 0, 1 &#125;, &#123; 2, 3 &#125;, &#123; 4, 5 &#125;, &#123; 6, 7 &#125;, &#123; 0, 2 &#125;, &#123; 1, 3 &#125;, &#123; 4, 6 &#125;, &#123; 5, 7 &#125;, &#123; 0, 4 &#125;, &#123; 1, 5 &#125;, &#123; 7, 3 &#125;, &#123; 2, 6 &#125;&#125;;static float rotate = 0;static int times = 0;GLint windowWidth = 800;GLint windowHeight = 800;GLfloat xRotAngle = -75.0f;GLfloat yRotAngle = 0.0f;GLfloat zRotAngle = -135.0f;float MIN_X = -200;float MAX_X = 200;float MIN_Y = -200;float MAX_Y = 200;float MIN_Z = -200;float MAX_Z = 200;GLfloat coordinatesize = 200.0f;GLfloat ratio = 1;void drawLine(float x1, float y1, float z1, float x2, float y2, float z2)&#123; glBegin(GL_LINES); glVertex3f(x1, y1, z1); glVertex3f(x2, y2, z2); glEnd(); glFlush();&#125;// 绘制立方体void DrawCube(void)&#123; int i, j; glBegin(GL_LINES); for (i = 0; i &lt; 12; ++i) // 12 条线段 &#123; for (j = 0; j &lt; 2; ++j) // 每条线段 2个顶点 &#123; glVertex3fv(vertex_list[index_list[i][j]]); &#125; &#125; glEnd(); glFlush();&#125;void reshapeOperate()&#123; glMatrixMode(GL_PROJECTION); glLoadIdentity(); if (ratio &lt; 1) glOrtho(-coordinatesize, coordinatesize, -coordinatesize / ratio, coordinatesize / ratio, -coordinatesize, coordinatesize); else glOrtho(-coordinatesize*ratio, coordinatesize*ratio, -coordinatesize, coordinatesize, -coordinatesize, coordinatesize); glMatrixMode(GL_MODELVIEW); glLoadIdentity();&#125;void reshape(int w, int h) &#123; if ((w == 0) || (h == 0)) return; glViewport(0, 0, w, h); ratio = (GLfloat)w / (GLfloat)h; reshapeOperate();&#125;GLfloat AngleX = 45.0f;GLfloat AngleY = 315.0f;void reshape1(int w, int h)&#123; GLfloat aspect = (GLfloat)w / (GLfloat)h; GLfloat nRange = 100.0f; glViewport(0, 0, w, h); glMatrixMode(GL_PROJECTION); //将当前矩阵指定为投影模式 glLoadIdentity(); //设置三维投影区 if (w &lt;= h) &#123; glOrtho(-nRange, nRange, -nRange * aspect, nRange * aspect, -nRange, nRange); &#125; else &#123; glOrtho(-nRange, nRange, -nRange / aspect, nRange / aspect, -nRange, nRange); &#125;&#125;void onDraw(void*)&#123; // Draw something using OpenGL here //glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); //清除所有的像素 //glMatrixMode(GL_MODELVIEW); //glLoadIdentity(); //glPushMatrix(); ////glTranslatef(-0.2, 0, 0); // 平移 ////glScalef(2, 1, 1); // 缩放 //glRotatef(xRotAngle, 1.0f, 0.0f, 0.0f); //glRotatef(yRotAngle, 0.0f, 1.0f, 0.0f); //glRotatef(zRotAngle, 0.0f, 0.0f, 1.0f); //glColor3f(1, 0, 0); //drawLine(0, 0, 0, MAX_X, 0, 0); //x轴 //glColor3f(0, 1, 0); //drawLine(0, 0, 0, 0, MAX_Y, 0); //y轴 //glColor3f(0, 0, 1); //drawLine(0, 0, 0, 0, 0, MAX_Z); //z轴 //times++; //if (times &gt; 1) //&#123; // times = 0; //&#125; //if (times % 1 == 0) //&#123; // rotate += 0.3; //&#125; //glRotatef(rotate, 0, 1, 0); //glRotatef(rotate, 1, 0, 0); //glColor3f(0, 1, 1); //DrawCube(); //glPopMatrix(); reshape1(windowWidth, windowHeight); glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT); glMatrixMode(GL_MODELVIEW); glLoadIdentity(); AngleX++; AngleY++; glPushMatrix(); &#123; glRotatef(AngleX, 1.0f, 0.0f, 0.0f); glRotatef(AngleY, 0.0f, 1.0f, 0.0f); glBegin(GL_POLYGON); //前表面 glColor3ub((GLubyte)255, (GLubyte)255, (GLubyte)255);//颜色设置为白色 glVertex3f(50.0f, 50.0f, 50.0f); glColor3ub((GLubyte)255, (GLubyte)255, (GLubyte)0);//颜色设置为黄色 glVertex3f(50.0f, -50.0f, 50.0f); glColor3ub((GLubyte)255, (GLubyte)0, (GLubyte)0);//颜色设置为红色 glVertex3f(-50.0f, -50.0f, 50.0f); glColor3ub((GLubyte)255, (GLubyte)0, (GLubyte)255);//颜色设置为白色 glVertex3f(-50.0f, 50.0f, 50.0f); glEnd(); glBegin(GL_POLYGON); //后表面 glColor3f(0.0f, 1.0f, 1.0f);//颜色设置为青色 glVertex3f(50.0f, 50.0f, -50.0f); glColor3f(0.0f, 1.0f, 0.0f);//颜色设置为绿色 glVertex3f(50.0f, -50.0f, -50.0f); glColor3f(0.0f, 0.0f, 0.0f);//颜色设置为黑色 glVertex3f(-50.0f, -50.0f, -50.0f); glColor3f(0.0f, 0.0f, 1.0f);//颜色设置为蓝色 glVertex3f(-50.0f, 50.0f, -50.0f); glEnd(); glBegin(GL_POLYGON); //上表面 glColor3d(0.0, 1.0, 1.0);//颜色设置为青色 glVertex3f(50.0f, 50.0f, -50.0f); glColor3d(1.0, 1.0, 1.0);//颜色设置为白色 glVertex3f(50.0f, 50.0f, 50.0f); glColor3d(1.0, 0.0, 1.0);//颜色设置为品红色 glVertex3f(-50.0f, 50.0f, 50.0f); glColor3d(0.0, 0.0, 1.0);//颜色设置为蓝色 glVertex3f(-50.0f, 50.0f, -50.0f); glEnd(); glBegin(GL_POLYGON); //下表面 glColor3ub(0u, 255u, 0u);//颜色设置为绿色 glVertex3f(50.0f, -50.0f, -50.0f); glColor3ub(255u, 255u, 0u);//颜色设置为黄色 glVertex3f(50.0f, -50.0f, 50.0f); glColor3ub(255u, 0u, 0u);//颜色设置为红色 glVertex3f(-50.0f, -50.0f, 50.0f); glColor3ub(0u, 0u, 0u);//颜色设置为黑色 glVertex3f(-50.0f, -50.0f, -50.0f); glEnd(); glBegin(GL_POLYGON); //左表面 glColor3ub((GLubyte)255, (GLubyte)255, (GLubyte)255);//颜色设置为白色 glVertex3f(50.0f, 50.0f, 50.0f); glColor3ub((GLubyte)0, (GLubyte)255, (GLubyte)255);//颜色设置为青色 glVertex3f(50.0f, 50.0f, -50.0f); glColor3ub((GLubyte)0, (GLubyte)255, (GLubyte)0);//颜色设置为绿色 glVertex3f(50.0f, -50.0f, -50.0f); glColor3ub((GLubyte)255, (GLubyte)255, (GLubyte)0);//颜色设置为黄色 glVertex3f(50.0f, -50.0f, 50.0f); glEnd(); glBegin(GL_POLYGON); //右表面 glColor3f(1.0f, 0.0f, 1.0f);//颜色设置为品红色 glVertex3f(-50.0f, 50.0f, 50.0f); glColor3f(0.0f, 0.0f, 1.0f);//颜色设置为蓝色 glVertex3f(-50.0f, 50.0f, -50.0f); glColor3f(0.0f, 0.0f, 0.0f);//颜色设置为黑色 glVertex3f(-50.0f, -50.0f, -50.0f); glColor3f(1.0f, 0.0f, 0.0f);//颜色设置为红色 glVertex3f(-50.0f, -50.0f, 50.0f); glEnd(); &#125; glPopMatrix();&#125;void opencvWithOpenGLTest()&#123; std::string openGLWindowName = "OpenGL Test"; cv::namedWindow(openGLWindowName, cv::WINDOW_OPENGL); cv::resizeWindow(openGLWindowName, windowWidth, windowHeight); cv::setOpenGlContext(openGLWindowName); cv::setOpenGlDrawCallback(openGLWindowName, onDraw, NULL); while (cv::waitKey(30) != 27) &#123; cv::updateWindow(openGLWindowName); // when needed &#125;&#125;int main(int argc, char *argv[])&#123; opencvWithOpenGLTest(); return 0;&#125; 运行成功后可看到一个旋转的彩色立方体。 结论 从实验结果来看，OpenCV确实能支持OpenGL图形的显示，但其不足之处也很明显：没有提供鼠标和键盘的交互操作（可能是本人还没发现，毕竟只是尝试一下看它能不能显示），仅仅只是提供一个显示窗口。如果真想用OpenGL做一些好玩的东西，还是用glut和glew吧，不过glut已经停止更新许久，glew在调试时可能会出现一些莫名其妙的错误，所以网上有人用freeglut代替glut，glee代替glew，具体的东西本人也没试过，本人目前还没做过OpenGL相关的事，这次用OpenCV显示OpenGL图形纯粹是为了好玩(*&#94;__&#94; *) 嘻嘻……。 后记 本篇文档也是上次编译配置完OpenCV-3.2后做的一次小实验，但当时并没有记录，所以还有一些参考资料也已经不知道了:-(。 参考资料[1] OpenCV学习笔记（六十一）——建立支持OpenGL的OpenCV工程“Master OpenCV”chp.3（http://blog.csdn.net/yang_xian521/article/category/910716 ） [2] 几个opengl立方体绘制案例（http://blog.csdn.net/bcbobo21cn/article/category/3104565 ）]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>opencv</tag>
        <tag>opengl</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Win10＋VS2013＋CMake-gui编译和配置OpenCV-3.2.0]]></title>
    <url>%2F2017%2F08%2F28%2FWin10%EF%BC%8BVS2013%EF%BC%8BCMake-gui%E7%BC%96%E8%AF%91%E5%92%8C%E9%85%8D%E7%BD%AEOpenCV-3-2-0%2F</url>
    <content type="text"><![CDATA[本人系统环境：Win10_x64 英文企业版；VS2013-update5 英文旗舰版；CMake-3.6.3-win64-x64 免安装版；Qt-opensource-windows-x86-msvc2013-5.6.2。 *注：本人写的这篇文档主要用来编译x86版的动态debug库，想编译其它类型的库请自行参考其它资料，做相关改变。（其实如果想编译x64版的可以在用VS2013编译时将上方的Win32平台选择x64平台；想编译release版的可以在用VS2013编译时将上方的Debug模式选择Release模式；想编译静态库的可以在用CMake生成时取消勾选BUILD_SHARED_LIBS选项即可。:-P） 前言 因为OpenCV-3.2官方的release版只有支持VS2015的库，而且不包括扩展包（opencv_contrib）中的库，而由于某些历史原因，本人目前使用的编译器还是VS2013，又想用用扩展包中一些有趣的算法，在加上上个月opencv-3.3还没有正式release，所以上个月本人就利用VS2013对opencv-3.2进行编译。具体编译过程如下： 准备篇 先在GitHub上下载对应的opencv源码包：opencv-3.2.0和opencv_contrib-3.2.0（https://github.com/opencv ），扩展包版本一定要和opencv版本相同。本人为了添加Qt后端显示支持（为了好看和方便:-P），所以还下载安装了支持VS2013的Qt-5.6.2（http://download.qt.io/archive/qt/ ）。再下载CMake-3.6.3-win64-x64 免安装版（https://cmake.org/files/ ）。至于微软的东西，推荐直接去MSDN 我告诉你去下载。 由于网上有的资料（具体是哪篇文章本人忘记了o(╯□╰)o）说编译时的文件结构可能会影响编译是否成功，再加上为了方便编译管理，本人的编译时的文件结构为： opencv-3.2.0_build├── build└── sources​ ├── opencv-3.2.0​ └── opencv_contrib-3.2.0 4 directories, 0 files 其中opencv-3.2.0用来装opencv-3.2.0.zip解压后的源码；opencv_contrib-3.2.0用来装opencv_contrib-3.2.0.zip解压后的源码；build用来装CMake编译完成后的文件。 编译篇 打开/cmake-3.6.3-win64-x64/bin/cmake-gui.exe，在Where is the source code文本框中选择/opencv-3.2.0_build/sources/opencv-3.2.0；在Where to build the binaris文本框中选择/opencv-3.2.0_build/build，点击Configure，在弹出的编译器选择框中选择Visual Studio 12 2013，一直Configure直到红色的条变白。 网上有人在这一步可能会出现ffmpeg not downloaded和“ippicv_windows_20151201.zip”not downloaded这两个问题，本人没出现这两个问题，所以没有机会验证cmake-gui和vs2013编译opencv和opencv_contrib源码中的解决办法是否正确。 接下来就是添加扩展包，在白色条中找到OPENCV_EXTRA_MODULES_PATH文本框，在其中选择opencv_contrib源码中modeles所在路径：/opencv-3.2.0_build/sources/opencv_contrib-3.2.0/modules。 至于想要支持OpenGL和Qt就需要勾选WITH_OPENGL和WITH_QT并Configure后选择好Qt的安装目录，如果配置好Qt的环境变量Cmake将会自动选择好Qt所在路径。 随后再次反复Configure直到界面不再出现红色背景，之后单击Generate。不出意外的话，你会看到Configure done和Generate done。 本人在这一步出现了VS2013_CMake_opencv3.1动态库与静态库的配置与编译中的问题，原因是同时勾选了同时勾选了BUILD_opencv_world和BUILD_opencv_contirb_world，本人的解决办法是将它们全部取消勾选，再次Configure和Generate。 如果上面一切顺利的话就可以进行下一步了：使用VS2013编译OpenCV。打开/opencv-3.2.0_build/build目录，将会看到一大堆文件和文件夹，双击/opencv-3.2.0_build/build目录下的OpenCV.sln，用VS2013打开。找到CMakeTargets中的INSTALL，然后右键选择“Project Only”–&gt;“Build Only INSTALL”。 漫长的等待。。。。。。(╯﹏╰)b 本人在这一步出现了一个问题，具体问题和解决方法详见问题篇。 一切顺利的话，应该会比本人下面的库多两个，本人最后生成的Debug库为： opencv_aruco320d.lib opencv_bgsegm320d.lib opencv_bioinspired320d.lib opencv_calib3d320d.lib opencv_ccalib320d.lib opencv_core320d.lib opencv_datasets320d.lib opencv_dnn320d.lib opencv_dpm320d.lib opencv_face320d.lib opencv_features2d320d.lib opencv_flann320d.lib opencv_fuzzy320d.lib opencv_highgui320d.lib opencv_imgcodecs320d.lib opencv_imgproc320d.lib opencv_line_descriptor320d.lib opencv_ml320d.lib opencv_objdetect320d.lib opencv_optflow320d.lib opencv_phase_unwrapping320d.lib opencv_photo320d.lib opencv_plot320d.lib opencv_reg320d.lib opencv_rgbd320d.lib opencv_saliency320d.lib opencv_shape320d.lib opencv_stereo320d.lib opencv_stitching320d.lib opencv_structured_light320d.lib opencv_superres320d.lib opencv_surface_matching320d.lib opencv_text320d.lib opencv_tracking320d.lib opencv_video320d.lib opencv_videoio320d.lib opencv_videostab320d.lib opencv_xfeatures2d320d.lib opencv_ximgproc320d.lib opencv_xobjdetect320d.lib opencv_xphoto320d.lib 共41个。 配置篇 因为本人只编译了x86版动态debug库，所以以下环境配置都只针对x86版动态debug库。（其实要配置x64的库就只需将x86换成x64即可；要配置release模式的库就只需在添加附加依赖项中的库文件选择release模式的库（即数字后没有d的lib）；若要配置静态库就需要选择静态库文件夹以及在附加依赖项中添加相应的静态库文件。:-P） 首先把/opencv-3.2.0_build/build/install中的文件都提取出来，这和OpenCV官方release的opencv文件结构差不多，具体两层结构如下 .├── bin│ &ensp;└── opencv_waldboost_detectord.exe├── etc│ &ensp;├── haarcascades│ &ensp;└── lbpcascades├── include│ &ensp;├── opencv│ &ensp;└── opencv2├── LICENSE├── OpenCVConfig.cmake├── OpenCVConfig-version.cmake└── x86​ └── vc12 9 directories, 4 files x86文件夹就是VS2013生成的对应VS版本32位的各种库，include文件夹就是opencv的各项模块。本人将其中提取出的文件全部放入了C:\Program Files\OpenCV\3.2.0\build文件夹中。 首先配置环境变量，系统（或用户）环境变量如下： 变量名 变量值 Path C:\Program Files\OpenCV\3.2.0\build\x86\vc12\bin OPENCV C:\Program Files\OpenCV\3.2.0\build 不然可能会报错：程序“XXXXXX”已退出，返回值为 -1073741701 (0xc000007b)。其中下面那行可以选择不要添加。 然后在VS中配置环境。新建工程，然后在“属性管理器”中对应项目下Debug | Win32文件夹右键“添加新项目属性表”。（方便一次配置，多次使用，以后再使用只要在相应项目下右键“添加现有属性表”即可），本人新项目属性表取名为：opencv-3.2.0_msvc2013_x86d.props。 接下来就是真正的VS环境配置了： 双击打开刚才新建的属性表，选中“VC++目录”，注意在进行以下配置时建议都勾选左下角的“从父级或项目默认设置继承” “可执行文件目录”中添加： C:\Program Files\OpenCV\3.2.0\build\x86\vc12\bin “包含目录”中添加： C:\Program Files\OpenCV\3.2.0\build\include C:\Program Files\OpenCV\3.2.0\build\include\opencv C:\Program Files\OpenCV\3.2.0\build\include\opencv2 “库目录”中添加： C:\Program Files\OpenCV\3.2.0\build\x86\vc12\lib 选中“链接器” –&gt; “常规”，“附加库目录”中添加： C:\Program Files\OpenCV\3.2.0\build\x86\vc12\lib “链接器” –&gt; “输入”，“附加依赖项”中添加C:\Program Files\OpenCV\3.2.0\build\x86\vc12\lib中数字后带d的库文件，即编译篇中本人最后生成的41个库文件。 配置完之后不要忘了右键该属性表进行保存处理，以便下个项目直接使用，不需要再重复进行配置。 最后附示例程序： 1234567891011#include &lt;opencv2/opencv.hpp&gt; int main(int argc, char *argv[])&#123; cv::Mat lena = cv::imread("lena.jpg"); //载入图像到Mat，jpg文件和该cpp在同一文件夹 cv::namedWindow("lena"); //创建一个名为 "lean"的窗口 cv::imshow("lena", lena); //显示名为 "lena"的窗口 cv::waitKey(5000); // 只对窗口机制起作用（显示5000ms，随后返回-1，即窗口关闭），若在此期间有按键按下，则马上返回按键的ASCII码。 //system("pause"); return 0;&#125; 这里必须在imshow后加入waitkey，因为WaitKey不止是Wait Key 而已，它其实还涉及到消息响应，有这个函数cv内部的WndProc函数才能起作用，才会更新窗口。 最后程序运行成功并显示lena图，则说明编译和配置没问题。 问题篇1、用VS2013编译OpenCV在漫长的等待阶段出现的问题。 问题描述：CVV模块报错，TS模块编译不出来，好在这两个模块都不是很重要，可以忽略，本人强迫症也没到这种程度O(∩_∩)O~。 解决办法： 在CVV模块报错后可在CMake（不知道具体是INSTALL下的CMake Rules中的INSTALL_force.rule，还是ALL_BUILD下的CMakeLists.txt，忘记了o(╯□╰)o）中添加-DBUILD_opencv_cvv=OFF忽略CVV模块，从而正常编译其它模块。参考errors on build opencv with cvv module and qt5 #577。如果实在不行的话就在CMake生成的时候取消勾选出错模块，若是用CMake重新生成的话不要忘了先把/opencv-3.2.0_build/build目录下的文件全部删除干净。 后记 这是以前写的两篇文档，现在再来整理成一篇。 附录 既然能看到这里，说明是想在VS下使用OpenCV，这里推荐一款VS下OpenCV开发调试神器：Image Watch，效果谁用谁知道。Image Watch是VS的一个插件，不过它只支持VS2012及以上版本。使用方法为先设置断点（F9），随后在调试（F5）模式下，鼠标指针悬停在cv::Mat类型变量上，即可出现，点击查看图标即可显示相应图像。 参考资料[1] cmake-gui和vs2013编译opencv和opencv_contrib源码（http://livezingy.com/category/opencv/ ） [2] VS2013_CMake_opencv3.1动态库与静态库的配置与编译（http://livezingy.com/category/opencv/ ） [3] 使用VS2015编译以及静态编译opencv3记录 [4] errors on build opencv with cvv module and qt5 #577 [5] VS2013中Image Watch插件的使用(OpenCV)（http://blog.csdn.net/fengbingchun/article/category/721609 ）]]></content>
      <categories>
        <category>Study</category>
      </categories>
      <tags>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[解决写上篇文档“Hexo+GitHub搭建个人博客”遇到的问题]]></title>
    <url>%2F2017%2F08%2F26%2F%E8%A7%A3%E5%86%B3%E5%86%99%E4%B8%8A%E7%AF%87%E6%96%87%E6%A1%A3%E2%80%9CHexo%2BGitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E2%80%9D%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[超链接网址问题 问题描述：使用正常的markdown超链接格式[]()没问题，然而当直接将一个网址链接放入该文档时，它会将该链接后面的文字也当成是该链接的一部分，直接点击链接时，会将后面的文字也放入浏览器地址栏，从而出现网页404错误：404: Page could not be found。 解决办法：在网址链接后输入一个空格以隔开网址链接和后面的文字。 超链接样式问题 问题描述：本人使用的hexo主题是基于spfk主题稍微修改过的，spfk主题能自动修改超链接原有的样式，挺好看的:D，但是当本人在markdown中数字编号列表，即有序列表中添加超链接时，其样式并没有修改，还是普通的超链接样式。 解决办法：没有解决。最后只是跳过了这个问题，就用中文的序号表示列表。 文本段落问题 问题描述：为了使文本有段落感，一般都会在段落首字前空两格，但是在markdown中空两格，用hexo发布后并没有空两格，这使得文档没有段落感，阅读体验有点差。 解决办法：将中文输入法由半角切换至全角，在段落首字前输入两个空格即可。 显示英文尖括号问题 问题描述：由于上篇文档需要在文档中显示&lt;youname&gt;，但由于Hexo可能将其当做一个xml标签处理了，所以发布之后的文档没有显示该文字。 解决办法：首先本人尝试了转义字符\，谁曾想它只出现了一个转义字符，该文字还是没显示，本人差点又要跳过这个问题，将其用另一种表示法了。后来本人想到这最后不是会转为html吗，本人就直接用html中尖括号的表示法不就行啦:p，于是参考HTML语言中括号(尖括号)的字符编码，用&amp;lt;代替&lt;，用&amp;gt;代替&gt;，最后该文字终于出来了。 给文字添加颜色问题 问题描述：本人想给注意事项上的需要注意的问题添加醒目的颜色，但markdown本身不支持给文字添加颜色。 解决办法：由于Hexo最后会将markdown文档转换为html文档发布，所以直接将html标签写进markdown文档，最后自然会出现html样式，本人这里参考CSDN-markdown编辑器语法——字体、字号与颜色，给想要变色的文字添加&lt;font color=#FA8072&gt;&lt;/font&gt;标签。 参考资料[1] HTML语言中括号(尖括号)的字符编码（http://liuxufei.com/weblog/jishu ） [2] CSDN-markdown编辑器语法——字体、字号与颜色（http://blog.csdn.net/testcs_dn ）]]></content>
      <categories>
        <category>Problems</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub搭建个人博客]]></title>
    <url>%2F2017%2F08%2F26%2FHexo%2BGitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[本人系统环境：Win10_x64。 前言 本来是想在国内某网站上继续写的，毕竟完全不需要自己管理，只需要负责写好文档就可以了，但某一天，该网站由于响应国家的号召，要实名验证，本来实名验证也没什么，就输入手机号，并填写验证码即可，但该网站实名验证的方式给人的感觉特别不爽，于是就决定自己搭建博客，这样虽然有点麻烦，但由于完全是自己管理，自己完全拥有该文档的所有权，也不用担心哪天别的网站突然出现的各种破问题，相比这种完全自由支配、无比爽快的感觉，管理这种麻烦就是小事了。 GitHub+Hexo个人博客搭建准备篇在GitHub上搭建博客的要求： 1、要有GitHub账号。（没有怎么办，没有就去注册啊） 使用Hexo框架的要求： 1、需要安装node.js。（电脑上没有安装怎么办，没有安装就去下载（https://nodejs.org/en/download/ ）安装啊） 2、需要安装git。（没有安装就去下载安装，附git学习教程） GitHub篇 满足上文的要求之后，就可以开始搭建了，首先在GitHub中新建一个仓库（New repository），在Repository name下填写&lt;yourname&gt;.github.io，其它可默认，点击Create repository。 新建仓库完成后，点击Create new file新建一个README.md文件，随便写点什么，比如“It&#39;s my blog website”。 点击上方横条选项中的Settings，查看GitHub Pages里的设置，上方应该有绿色框，框中“Your site is published at https://&lt;yourname&gt;.github.io”，该网址即为博客主页，Source应该是master branch，自此GitHub上的设置可以算是完成了，但为了方便和防止误删，一般把Hexo文件也放入GitHub中，为方便管理，可以新建另一分支专门放Hexo文件。 在仓库code界面中点击Branch：master，在出现的框中输入hexo新建hexo分支，在branches中Change default branch设置hexo为默认分支。 Hexo篇 将刚才新建的仓库克隆到本地：git clone https://github.com/&lt;yourname&gt;/&lt;yourname&gt;.github.io.git当前在hexo分支。 在&lt;yourname&gt;.github.io文件夹下执行 12345npm install hexo-cli -ghexo init blogcd blognpm installnpm install hexo-deployer-git --save 按这样一连串执行，如果没出问题的话就会在&lt;yourname&gt;.github.io文件夹里生成一个blog文件夹，该文件夹有一大堆Hexo有关的文件。 配置Hexo Hexo的配置文件为blog文件夹中的_config.yml文件。 修改配置文件不要使用windows自带的记事本，本人使用的VS Code，或者Notepad++和Sublime Text 2等编辑器都可以，以防文件编码改变，具体修改如下： 123456789101112131415# Sitetitle: &lt;你的blog名&gt;author: &lt;作者名称&gt;language: zh-CN&lt;网站所用语言，中国大陆选择zh-CN即可&gt;# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: http://&lt;yourname&gt;.github.io# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/&lt;yourname&gt;/&lt;yourname&gt;.github.io.git branch: master 其它的默认即可，具体参数信息详见Hexo官方文档。 配置git用户信息12git config --global user.name "&lt;yourname&gt;"git config --global user.email "&lt;yourname&gt;@xxxxxx.com" 如果是个人电脑的话推荐加上–global全局参数，因为这样更加方便，如果不加的话，还要在\&lt;yourname&gt;.github.io\blog\.deploy_git\.git中config里加入git用户信息，不然可能提交会出问题，稍显麻烦。 部署Hexo在blog文件夹下执行： 123hexo g #generate 生成静态文件hexo d #deploy 部署网站.部署网站前,需要预先生成静态文件hexo s #server 启动服务器 或者执行： 1hexo g -d 快速部署个人blog。 在浏览器中输入http://localhost:4000/，将会出现Hexo的Hello World界面，更多Hexo命令详见Hexo官方文档。 最后将Hexo文件提交到GitHub远程仓库，具体提交命令为： 123git add .git commitgit push origin hexo 在浏览器中输入https://&lt;yourname&gt;.github.io同样会出现Hexo的Hello World界面，自此整个个人blog的框架已经完全搭好了。 其它篇主题选择 主题可以去官网上的主题界面去找，目前比较受欢迎主题有next和yilia，去别人GitHub上的主题仓库上去下载或clone均可，本人目前用的主题为black-blue，这个主题本人在用的时候还有些问题，或许会换，或许会自己魔改。最后由于术业有专攻，实在不知道该改哪里，所以决定换black-blue的原版主题SPFK ，对照着black-blue对spfk进行修改。具体换主题的方法为： 先将下载好的主题整个放在\blog\themes文件夹中，再修改blog文件夹中的配置文件_config.yml： 12# theme: landscapetheme: black-blue black-blue为打包主题文件并放入\blog\themes文件夹中的文件夹名，并不是原主题名，只是本人恰好将其重命名为主题名。 文章发布发布文章需要在blog文件夹中执行： 1hexo new "test" 将会在\blog\source_posts\文件夹中生成test.md文件，随后编辑test.md文件即可，本人使用的Markdown编辑器为Typora。 至于给文章打标签和分类什么的，请参考Hexo官方文档。 写完文章之后推送到GitHub中，需要执行： 123git add .git commit -m "add test.md"git push origin hexo Hexo文件配置同样需要同步一下： 123hexo ghexo cleanhexo d 插件添加以RSS订阅插件为例。首先安装hexo-generator-feed： 12## rss插件npm install hexo-generator-feed --save 安装成功后，修改blog文件夹中的配置文件_config.yml： 1234# Extensions## Plugins: https://hexo.io/plugins/plugin:- hexo-generator-feed #RSS订阅 最后，修改当前主题文件夹中的配置文件_config.yml，添加RSS订阅链接即可： 12subnav: rss: "/atom.xml" 修改完成后，执行 123hexo cleanhexo ghexo d 将会在页面中看到RSS图标。 注意事项1、提交至远程仓库时可能会出现错误。 原因可能是因为没有将SSH Key添加到GitHub中。 查看当前用户主目录下的.ssh文件夹中（windows是C:\Users\&lt;username&gt;\.ssh）是否有id_rsa（私钥）和id_rsa.pub（公钥）这两个文件，若没有，则执行 1ssh-keygen -t rsa -C "youremail@example.com" 在GitHub中添加SSH Key的具体方法为：点击GitHub用户头像下的Settings，选中SSH and GPG keys，点击New SSH key，将id_rsa.pub中的内容复制粘贴到Key文本框中。 2、Hexo生成和部署命令都执行失败。 原因可能是修改配置文件_config.yml出错。 将修改的配置文件_config.yml复原试试。 3、Hexo部署之后网页没变化。 可能需要执行 1hexo clean 清除缓存文件 (db.json) 和已生成的静态文件 (public)。在某些情况（尤其是更换主题后），如果发现对站点的更改无论如何也不生效，可能需要运行该命令。 后记 以后就在这上面写blog了，顺便把以前写的一些文档也放上来。 参考资料[1] 利用github+hexo搭建自己的博客（http://blog.csdn.net/u012150360/article/category/6765461 ） [2] Hexo官方文档（https://hexo.io/zh-cn/ ） [3] GITHUB+HEXO博客轻松更换主题外观（http://www.jianshu.com/nb/10649566 ） [4] Hexo—正确添加RSS订阅（http://hanhailong.com/tags/Hexo%E4%B8%BB%E9%A2%98/ ）]]></content>
      <categories>
        <category>建站小记</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F08%2F26%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
